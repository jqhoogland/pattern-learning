{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "!pip install numpy torch sympy mod blobfile pandas seaborn matplotlib tqdm einops wand\n",
                "\n",
                "import sys\n",
                "import os\n",
                "\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
                "\n",
                "from contextlib import suppress\n",
                "from dataclasses import dataclass, asdict\n",
                "from datetime import datetime\n",
                "from typing import Callable, Literal, Optional, Union, Tuple, List\n",
                "from copy import deepcopy\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "from torch import nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader\n",
                "from torch import optim\n",
                "import wandb\n",
                "from tqdm.notebook import tqdm\n",
                "import ipywidgets as widgets\n",
                "import wandb\n",
                "\n",
                "import matplotlib as mpl\n",
                "from matplotlib.colors import LogNorm\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.ndimage import gaussian_filter\n",
                "\n",
                "from patterns.dataset import ModularArithmetic, Operator\n",
                "from patterns.transformer import Transformer\n",
                "from patterns.utils import generate_run_name\n",
                "from patterns.learner import Config\n",
                "from patterns.arithmetic.learner import ModularArithmeticLearner, ModularArithmeticConfig\n",
                "\n",
                "from unifying.sweep import get_history, handle_outliers, extract_run, extract_slice, get_pivot, extract_slice_from_pivot\n",
                "from unifying.plotting import plot, plot_slice, BLUE, RED, BLUES, REDS, plot_all_details, plot_details, create_heatmap\n",
                "from matplotlib import cm, colors, gridspec\n",
                "\n",
                "\n",
                "DEFAULT_MODULUS = 113\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "sns.set_theme(style=\"darkgrid\")\n",
                "ENTITY = \"<Insert entity here>\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sweeps\n",
                "\n",
                "To initialize a sweep, run the following command:\n",
                "\n",
                "```shell\n",
                "wandb sweep --project grokking <config.yml>\n",
                "```\n",
                "\n",
                "where `<config.yml>` is the config file you want to use.\n",
                "\n",
                "To run the sweep, run the following command:\n",
                "\n",
                "```shell\n",
                "wandb agent <sweep_id> --function train\n",
                "```\n",
                "\n",
                "where `<sweep_id>` is the id of the sweep you want to run. You can find the sweep id by running `wandb sweep ls`.\n",
                "\n",
                "You can pass an optional `--count` flag to the `wandb agent` command to specify the number of runs you want to execute. If you don't pass this flag, the agent will run until all the runs in the sweep are complete (for a grid sweep).\n",
                "\n",
                "On a multi-GPU machine, you can run multiple agents in parallel through the following:\n",
                "\n",
                "```shell\n",
                "CUDA_VISIBLE_DEVICES=0 wandb agent <sweep_id> &\n",
                "CUDA_VISIBLE_DEVICES=1 wandb agent <sweep_id> &\n",
                "...\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import numpy as np\n",
                "\n",
                "def generate_coarse_to_fine_grid_sweep(min_, max_, total_steps, step_sizes=[10, 5, 3, 1], type_=\"log\"):\n",
                "    if type_ == \"log\":\n",
                "        # Generate the logscale range\n",
                "        grid = np.logspace(np.log10(min_), np.log10(max_), total_steps)\n",
                "    elif type_ == \"linear\":\n",
                "        grid = np.linspace(min_, max_, total_steps)\n",
                "    else:\n",
                "        grid = np.arange(min_, max_, int((max_ - min_) / total_steps))\n",
                "\n",
                "    # Initialize an empty list to store the rearranged elements\n",
                "    rearranged_grid = []\n",
                "\n",
                "    # Iterate over the step sizes and merge the sublists\n",
                "    for step in step_sizes:\n",
                "        for i in range(0, len(grid), step):\n",
                "            if grid[i] not in rearranged_grid:\n",
                "                rearranged_grid.append(grid[i])\n",
                "\n",
                "    return rearranged_grid\n",
                "\n",
                "\n",
                "def rearrange_coarse_to_fine(grid: List, step_sizes=[10, 5, 3, 1]):\n",
                "    # Initialize an empty list to store the rearranged elements\n",
                "    rearranged_grid = []\n",
                "\n",
                "    # Iterate over the step sizes and merge the sublists\n",
                "    for step in step_sizes:\n",
                "        for i in range(0, len(grid), step):\n",
                "            if grid[i] not in rearranged_grid:\n",
                "                rearranged_grid.append(grid[i])\n",
                "\n",
                "    return rearranged_grid"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model-wise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_grid_1 = generate_coarse_to_fine_grid_sweep(1, 300, 31, step_sizes=[10, 5, 3, 1], type_=\"log\")\n",
                "\n",
                "# Drop duplicates and convert to int but maintain order\n",
                "model_grid_1 = list(dict.fromkeys((int(i) for i in model_grid_1)))\n",
                "\n",
                "model_grid_2 = generate_coarse_to_fine_grid_sweep(0, 300, 100, step_sizes=[10, 5, 3, 1], type_=\"range\")\n",
                "model_grid_2 = list(dict.fromkeys((int(i) for i in model_grid_2 if int(i) not in model_grid_1 and i)))\n",
                "print(len(model_grid_2), json.dumps(model_grid_2))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sample-wise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Frac train \n",
                "\n",
                "frac_train_grid = generate_coarse_to_fine_grid_sweep(0, 1.05, 20, step_sizes=[.5, .2, .1, .05], type_=\"range\")\n",
                "print(frac_train_grid)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Frac train \n",
                "mnist_frac_train_grid = generate_coarse_to_fine_grid_sweep(0.01, .10, 25, step_sizes=[10, 5, 3, 1], type_=\"log\")\n",
                "print(mnist_frac_train_grid)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Modulus\n",
                "\n",
                "import warnings\n",
                "\n",
                "\n",
                "def sieve_of_eratosthenes(n):\n",
                "    \"\"\"\n",
                "    Generate all prime numbers between 2 and n using the Sieve of Eratosthenes algorithm.\n",
                "    \"\"\"\n",
                "    # Create a boolean array \"prime[0..n]\" and initialize\n",
                "    # all entries it as true. A value in prime[i] will\n",
                "    # finally be false if i is Not a prime, else true.\n",
                "    prime = [True for i in range(n+1)]\n",
                "    p = 2\n",
                "    while p**2 <= n:\n",
                "        # If prime[p] is not changed, then it is a prime\n",
                "        if prime[p]:\n",
                "            # Update all multiples of p\n",
                "            for i in range(p**2, n+1, p):\n",
                "                prime[i] = False\n",
                "        p += 1\n",
                "\n",
                "    # Generate list of primes\n",
                "    primes = [p for p in range(2, n+1) if prime[p]]\n",
                "    return primes\n",
                "\n",
                "\n",
                "def primes_range(min_, max_, num_primes):\n",
                "    \"\"\"\n",
                "    Generate a list of primes between min_ and max_, \n",
                "    following a near-logarithmic scale with num_primes elements.\n",
                "    \"\"\"\n",
                "    all_primes = [p for p in sieve_of_eratosthenes(max_) if p >= min_]\n",
                "\n",
                "    # Generate the logscale range\n",
                "    grid = np.logspace(np.log10(min_), np.log10(max_), num_primes)\n",
                "\n",
                "    # Find the closest prime to each element in the grid\n",
                "    primes = []\n",
                "    used_indices = set()\n",
                "\n",
                "    for i in range(len(grid)):\n",
                "        # Find the index of the closest prime to the grid element\n",
                "        idx = np.argmin(np.abs(np.array(all_primes) - grid[i]))\n",
                "        \n",
                "        while idx in used_indices:\n",
                "            idx += 1\n",
                "\n",
                "        if idx >= len(all_primes):\n",
                "            warnings.warn(f\"Ran out of primes to choose from. Returning {len(primes)} primes.\")\n",
                "            break\n",
                "\n",
                "        used_indices.add(idx)\n",
                "        primes.append(all_primes[idx])\n",
                "\n",
                "    return primes\n",
                "\n",
                "modulus_grid = rearrange_coarse_to_fine(primes_range(1, 1000, 31), [10, 5, 3, 1])\n",
                "print(modulus_grid)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Regularization-wise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wds = generate_coarse_to_fine_grid_sweep(0.05, 10, 51)\n",
                "print(json.dumps(wds))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "frac_trains = generate_coarse_to_fine_grid_sweep(0.0167, 1., 15, step_sizes=[5, 3, 1], type_=\"log\")\n",
                "print(json.dumps(frac_trains))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lr_gains = generate_coarse_to_fine_grid_sweep(.4, 2.5, 15, step_sizes=[5, 3, 1], type_=\"log\")\n",
                "print(json.dumps(lr_gains))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Extensions"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Division"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MW_SWEEPS = [\"xkfsr247\"] # [\"i9ary9rm\"] # [\"l1b2mmci\", \"662klo00\"]\n",
                "# SW_SWEEPS = [\"s0iyilba\"]  # TODO: Redo for division\n",
                "# RW_SWEEPS = [\"ib21hnk1\"]\n",
                "# MW_SWEEPS = [\"xkfsr247\"] # [\"i9ary9rm\"] # [\"l1b2mmci\", \"662klo00\"]\n",
                "SW_SWEEPS = [\"s0iyilba\"]  # TODO: Redo for division\n",
                "RW_SWEEPS = [\"ib21hnk1\"]\n",
                "\n",
                "# Model-wise\n",
                "# mw_grokking = get_history(*MW_SWEEPS, unique_cols=\"d_model\")\n",
                "# mw_grokking_cleaned = handle_outliers(\n",
                "#     mw_grokking,\n",
                "#     loss_cols=[\"train/loss\"], \n",
                "#     action=\"keep\",\n",
                "#     unique_cols=[\"d_model\"],\n",
                "#     threshold=0.0001,\n",
                "#     late_epochs_ratio=0.6,\n",
                "# )\n",
                "\n",
                "\n",
                "# HACK TODO: Finish runs\n",
                "# min_d_model = mw_grokking_cleaned.d_model.min()\n",
                "# mw_grokking_cleaned.loc[mw_grokking_cleaned.d_model == min_d_model, \"train/acc\"] = 0.0\n",
                "# mw_grokking_cleaned.loc[mw_grokking_cleaned.d_model == min_d_model, \"test/acc\"] = 0.0\n",
                "\n",
                "# Sample-wise\n",
                "sw_grokking = get_history(*SW_SWEEPS, unique_cols=\"frac_train\")\n",
                "sw_grokking_cleaned = handle_outliers(\n",
                "    sw_grokking,\n",
                "    loss_cols=[\"train/loss\"], \n",
                "    action=\"keep\",\n",
                "    unique_cols=[\"frac_train\"],\n",
                "    threshold=0.0001,\n",
                "    late_epochs_ratio=0.6,\n",
                ")\n",
                "\n",
                "\n",
                "NUM_STEPS = sw_grokking._step.max()\n",
                "\n",
                "# Insert a column for frac_train = 0 with all zeros\n",
                "# This is true but kind of gross (You can assign 0 accuracy or 1 accuracy when you train on 0% of the data)\n",
                "# sw_grokking_cleaned = sw_grokking_cleaned.append(\n",
                "#     pd.DataFrame(\n",
                "#         {\n",
                "#             \"train/loss\": [0.0] * NUM_STEPS,\n",
                "#             \"test/loss\": [0.0] * NUM_STEPS,\n",
                "#             \"train/acc\": [0.0] * NUM_STEPS,\n",
                "#             \"test/acc\": [0.0] * NUM_STEPS,\n",
                "#             \"frac_train\": [0.0] * NUM_STEPS,\n",
                "#             \"_step\": range(1, NUM_STEPS),\n",
                "#         }\n",
                "#     )\n",
                "# )\n",
                "\n",
                "# Regularization-Wise \n",
                "rw_grokking = get_history(*RW_SWEEPS, unique_cols=\"weight_decay\")\n",
                "rw_grokking_cleaned = handle_outliers(\n",
                "    rw_grokking,\n",
                "    loss_cols=[\"train/loss\"], \n",
                "    action=\"remove\",\n",
                "    unique_cols=[\"weight_decay\"],\n",
                "    threshold=0.0001,\n",
                "    late_epochs_ratio=0.6,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate the 1x4 grid of Epoch-wise, Model-wise, Sample-wise, and Regularization-wise plots\n",
                "\n",
                "\n",
                "\n",
                "# Epoch-wise\n",
                "wd_selection = rw_grokking.weight_decay.unique()[10]\n",
                "steps, ew_slice = extract_run(rw_grokking_cleaned, weight_decay=wd_selection)\n",
                "\n",
                "# Extensions\n",
                "# mw_pivot_train = get_pivot(mw_grokking_cleaned, \"d_model\", [\"train/acc\"], reindex=True, interpolate=True, )\n",
                "# mw_train_slice = extract_slice_from_pivot(mw_pivot_train, 20000, \"train/acc\", \"d_model\", smooth=2.)\n",
                "\n",
                "# mw_pivot_test = get_pivot(mw_grokking_cleaned, \"d_model\", [\"test/acc\"], reindex=True, interpolate=True, )\n",
                "# mw_test_slice = extract_slice_from_pivot(mw_pivot_test, 20000, \"test/acc\", \"d_model\", smooth=2.)\n",
                "\n",
                "# mw_slice = {\n",
                "#     \"train/acc\": mw_train_slice[\"train/acc\"],\n",
                "#     \"test/acc\": mw_test_slice[\"test/acc\"],\n",
                "# }\n",
                "# d_model_vals = mw_test_slice.d_model.unique()\n",
                "\n",
                "frac_train_vals, sw_slice = extract_slice(sw_grokking_cleaned, 10000, \"frac_train\")\n",
                "wd_vals, rw_slice = extract_slice(rw_grokking_cleaned, 10000, \"weight_decay\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from matplotlib import gridspec \n",
                "\n",
                "def extract_slice_from_pivot(pivot_table, step, metric: Union[str, List[str]], unique_col, smooth: Union[bool, float]=False):\n",
                "    _pivot_table = pivot_table.copy()\n",
                "\n",
                "    if smooth:\n",
                "        _pivot_table[metric] = gaussian_filter(pivot_table[metric].values, sigma=smooth)\n",
                "\n",
                "    slice_ = _pivot_table.loc[_pivot_table.index == step, :].T.reset_index()\n",
                "    slice_ = pd.melt(\n",
                "        slice_,\n",
                "        id_vars=[unique_col, \"level_0\"],\n",
                "        var_name=\"_step\",\n",
                "        value_name=metric,\n",
                "    )\n",
                "\n",
                "    return slice_\n",
                "\n",
                "\n",
                "# Heatmap\n",
                "def create_heatmap(\n",
                "    x,\n",
                "    y,\n",
                "    z,\n",
                "    ax,\n",
                "    smooth: Union[bool, float] = False,\n",
                "    cmap=\"inferno\",\n",
                "    log_x: bool = True,\n",
                "    log_y: bool = True,\n",
                "    log_z: bool = True,\n",
                "    metric_label: str = \"\",\n",
                "    title: str = \"\",\n",
                "    flip: bool = True,\n",
                "):\n",
                "    X, Y = np.meshgrid(x, y)\n",
                "\n",
                "    if smooth:\n",
                "        z = gaussian_filter(z, sigma=smooth)\n",
                "\n",
                "    if log_z:\n",
                "        mesh = ax.pcolormesh(X, Y, z, cmap=cmap, norm=LogNorm())\n",
                "    else:\n",
                "        mesh = ax.pcolormesh(X, Y, z, cmap=cmap)\n",
                "\n",
                "    if log_y:\n",
                "        ax.set_yscale(\"log\")\n",
                "\n",
                "    if log_x:\n",
                "        ax.set_xscale(\"log\")\n",
                "\n",
                "    ax.set_title(metric_label)\n",
                "    ax.set_xlabel(title)\n",
                "    ax.set_ylabel(\"Steps\")\n",
                "    ax.set_ylim(y.max(), y.min())\n",
                "    ax.set_yticks([10**i for i in range(0, int(np.floor(np.log10(y.max()))))] + [y.max()])\n",
                "    ax.set_xlim(x[0], x[-1])\n",
                "\n",
                "    if flip:\n",
                "        ax.invert_yaxis()\n",
                "\n",
                "    return mesh\n",
                "\n",
                "def plot_details(\n",
                "    df: pd.DataFrame,\n",
                "    unique_col: str = \"weight_decay\",\n",
                "    smooth: Union[bool, float] = False,\n",
                "    log_loss=True,\n",
                "    cmap=\"inferno\",\n",
                "    log_x: bool = True,\n",
                "    log_y: bool = True,\n",
                "    log_time: bool = True,\n",
                "    title: str = \"\",\n",
                "    metric: str = \"test/acc\",\n",
                "    metric_label: str = \"Accuracy\",\n",
                "    step: int = 10000,\n",
                "    run_val: float = 0.0,\n",
                "    plot_extra: bool = False,\n",
                "    gs: Optional[Union[gridspec.GridSpec, gridspec.GridSpecFromSubplotSpec]] = None,\n",
                "    fig: Optional[plt.Figure] = None,\n",
                "    flip: bool = True,\n",
                "    xlabel: str = \"\"\n",
                "):\n",
                "    # Figure:\n",
                "    # - Top Row: Heatmap stretching across\n",
                "    # - Bottom Row: Two line plots.\n",
                "\n",
                "    metric_label_short = (\n",
                "        metric_label.split(\" \")[1] if \" \" in metric_label else metric_label\n",
                "    )\n",
                "\n",
                "        \n",
                "    # create a figure with a 2x2 grid of subplots\n",
                "    fig = fig or plt.figure(figsize=(10, 6))\n",
                "\n",
                "    gs = gs or gridspec.GridSpec(2, 4, width_ratios=[1.59, 6, 6, 1.59])\n",
                "    \n",
                "    ax1 = plt.subplot(gs[0, 1:])\n",
                "    ax2 = plt.subplot(gs[1, 1])\n",
                "    ax3 = plt.subplot(gs[1, 2])\n",
                "\n",
                "    pivot_table = get_pivot(\n",
                "        df, unique_col, reindex=True, interpolate=True, columns=[metric]\n",
                "    )\n",
                "    unique_vals = sorted(df[unique_col].unique())\n",
                "\n",
                "    xmin, xmax = unique_vals[0], unique_vals[-1]\n",
                "    xticks = [10**i for i in range(int(np.floor(np.log10(xmin))), int(np.ceil(np.log10(xmax)))) if 10**i >= xmin and 10**i <= xmax]\n",
                "\n",
                "    if xmin not in xticks:\n",
                "        xticks = [xmin, *xticks]\n",
                "    \n",
                "    if xmax not in xticks:\n",
                "        # Round to within 2 sigfigs\n",
                "        xmax = round(xmax, -int(np.floor(np.log10(xmax))) + 1)\n",
                "        xticks = [*xticks, xmax]\n",
                "\n",
                "    mesh = create_heatmap(\n",
                "        x=unique_vals,\n",
                "        y=pivot_table[metric].index,\n",
                "        z=pivot_table[metric].values,\n",
                "        ax=ax1,\n",
                "        smooth=smooth,\n",
                "        cmap=cmap,\n",
                "        log_x=log_x,\n",
                "        log_y=log_y,\n",
                "        log_z=log_loss and \"loss\" in metric,\n",
                "        title=xlabel,\n",
                "        metric_label=\"\",\n",
                "        flip=flip\n",
                "    )\n",
                "\n",
                "    ax1.set_ylabel(\"Steps\", fontsize=18)\n",
                "    ax1.set_xticks(xticks)\n",
                "    ax1.set_xticklabels(xticks)\n",
                "\n",
                "    fig.colorbar(mesh, ax=ax1)\n",
                "    \n",
                "    # Plot a vertical line at unique_col = run_val\n",
                "    ax1.axvline(x=run_val, color=BLUE, linestyle=\"--\", linewidth=1)\n",
                "\n",
                "    # Plot a horizontal line at step = step\n",
                "    ax1.axhline(y=step, color=RED, linestyle=\"--\", linewidth=1)\n",
                "\n",
                "    # Line plots\n",
                "\n",
                "    # Slice\n",
                "    slice_ = extract_slice_from_pivot(pivot_table, step, metric, unique_col, smooth=smooth)\n",
                "\n",
                "    if plot_extra:\n",
                "        num_steps = len(df._step.unique())\n",
                "        # Plot one slice every 100 steps\n",
                "        slices_table = pivot_table.loc[pivot_table.index % 10 == 0, :].T.reset_index()\n",
                "        slices = pd.melt(\n",
                "            slices_table,\n",
                "            id_vars=[unique_col, \"level_0\"],\n",
                "            var_name=\"_step\",\n",
                "            value_name=metric,\n",
                "        )\n",
                "\n",
                "        sns.lineplot(\n",
                "            data=slices,\n",
                "            x=unique_col,\n",
                "            y=metric,\n",
                "            hue=\"_step\",\n",
                "            ax=ax2,\n",
                "            alpha=100.0 / num_steps,\n",
                "            palette=REDS,\n",
                "            legend=False,\n",
                "        )\n",
                "        slice_norm = colors.Normalize(vmin=0, vmax=df._step.max())\n",
                "        slice_colorbar = cm.ScalarMappable(norm=slice_norm, cmap=REDS)\n",
                "        fig.colorbar(slice_colorbar, ax=ax2, label=\"Steps\")\n",
                "    \n",
                "    ax2.plot(unique_vals, slice_[metric], label=title, color=RED)\n",
                "    ax2.set_xlabel(xlabel, fontsize=18)\n",
                "    ax2.set_xticks(xticks)\n",
                "    ax2.set_xticklabels(xticks)\n",
                "\n",
                "    # Run example\n",
                "    kwargs = {unique_col: run_val}\n",
                "    df.sort_values(by=\"_step\", inplace=True)\n",
                "    steps, run = extract_run(df, **kwargs)\n",
                "\n",
                "    if plot_extra:\n",
                "        num_vals = len(unique_vals)\n",
                "        sns.lineplot(\n",
                "            data=df,\n",
                "            x=\"_step\",\n",
                "            y=metric,\n",
                "            hue=unique_col,\n",
                "            ax=ax3,\n",
                "            alpha=10.0 / num_vals,\n",
                "            palette=BLUES,\n",
                "            legend=False,\n",
                "        )\n",
                "        run_norm = colors.Normalize(vmin=min(unique_vals), vmax=max(unique_vals))\n",
                "        run_colorbar = cm.ScalarMappable(norm=run_norm, cmap=BLUES)\n",
                "        fig.colorbar(run_colorbar, ax=ax3, label=title)\n",
                "\n",
                "    run.dropna(inplace=True)\n",
                "    ax3.plot(steps, run[metric], label=title, color=BLUE, linestyle=\"-\")\n",
                "    ax3.set_xlabel(\"Steps\", fontsize=18)\n",
                "\n",
                "    for ax in [ax2, ax3]:\n",
                "        if \"Accuracy\" in metric_label:\n",
                "            ax.set_ylim(0.0, 1.05)\n",
                "        else:\n",
                "            min_loss, max_loss = df[metric].min(), df[metric].max()\n",
                "            \n",
                "            if log_loss:\n",
                "                ax.set_ylim(0.5 * min_loss, 1.5 * max_loss)\n",
                "                ax.set_yscale(\"log\")\n",
                "            else:\n",
                "                ax.set_ylim(-0.05 * max_loss, 1.05 * max_loss)\n",
                "\n",
                "        if log_x:\n",
                "            ax.set_xscale(\"log\")\n",
                "\n",
                "    if log_time:\n",
                "        ax3.set_xscale(\"log\")\n",
                "\n",
                "    ax2.set_ylabel(metric_label_short, fontsize=18)\n",
                "    ax3.set_yticklabels([])\n",
                "    ax3.set_ylabel(\"\")\n",
                "\n",
                "    # Adjust the layout of the subplots\n",
                "    # fig.suptitle(title)\n",
                "    # fig.tight_layout()\n",
                "\n",
                "    ax1.annotate(title, (0.5, 1), xytext=(0, 30), xycoords='axes fraction', textcoords='offset points', va='top', ha='center', fontsize=20)\n",
                "\n",
                "    # Fontsize\n",
                "    for ax in [ax1, ax2, ax3]:\n",
                "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
                "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=12)\n",
                "\n",
                "    return fig\n",
                "\n",
                "\n",
                "def plot_all_details(\n",
                "    df,\n",
                "    title,\n",
                "    unique_col,\n",
                "    run_val,\n",
                "    log_x=True,\n",
                "    log_y=True,\n",
                "    plot_extra=False,\n",
                "    cmap=\"viridis\",\n",
                "    metrics_and_labels=[\n",
                "        (\"train/acc\", \"Train Accuracy\"),\n",
                "        (\"test/acc\", \"Test Accuracy\"),\n",
                "        (\"train/loss\", \"Train Loss\"),\n",
                "        (\"test/loss\", \"Test Loss\"),\n",
                "    ],\n",
                "    format=\"png\",\n",
                "    **kwargs\n",
                "):\n",
                "    fig = plt.figure(figsize=(15, 10))\n",
                "    gs = gridspec.GridSpec(2, 2, hspace=0.5, wspace=0.025)\n",
                "\n",
                "    if run_val not in df[unique_col].values:\n",
                "        raise ValueError(f\"run_val={run_val} not in unique_vals={df[unique_col].values}\")\n",
                "\n",
                "    for i, (metric, label) in enumerate(metrics_and_labels):\n",
                "        I, J = i // 2, i % 2\n",
                "        sub_gs = gs[I, J].subgridspec(2, 4, width_ratios=[1.59, 6, 6, 1.59], hspace=0.75, wspace=0.15)\n",
                "\n",
                "        plot_details(\n",
                "            df,\n",
                "            metric_label=label,\n",
                "            metric=metric,\n",
                "            title=label,\n",
                "            xlabel=title,\n",
                "            unique_col=unique_col,\n",
                "            run_val=run_val,\n",
                "            cmap=cmap,\n",
                "            log_x=log_x,\n",
                "            log_y=log_y,\n",
                "            plot_extra=plot_extra,\n",
                "            gs=sub_gs,\n",
                "            fig=fig,\n",
                "            **kwargs\n",
                "        )\n",
                "    \n",
                "    fig.tight_layout(h_pad=0.5, w_pad=5)\n",
                "    fig.savefig(f\"../figures/{unique_col}.{format}\", dpi=300)\n",
                "    plt.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "api = wandb.Api()\n",
                "runs = api.runs(\"initially-overconf-learners/grokking\")\n",
                "\n",
                "desired_config = {\n",
                "    \"lr\": 0.001,\n",
                "    \"train_split\": 50,\n",
                "    \"weight_decay\": 0.00001,\n",
                "    \"num_heads\": 1,\n",
                "    \"num_layers\": 2,\n",
                "}\n",
                "\n",
                "def filter_config(config):\n",
                "    for k, v in desired_config.items():\n",
                "        if config.get(k) != v:\n",
                "            return False\n",
                "    \n",
                "    return True\n",
                "\n",
                "accepted = [r for r in runs if filter_config(r.config) and r.state == \"finished\" and \"train_step\" in r.summary and r.summary[\"train_step\"] == r.config[\"optimization_budget\"]]\n",
                "rejects = [r for r in runs if r not in accepted]\n",
                "\n",
                "len(accepted), len(rejects)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "KEYS = [\"Accurcy/train\", \"Accuracy/val\", \"Loss/train\", \"Loss/val\", \"train_step\"]\n",
                "\n",
                "def get_history(run):\n",
                "    \"\"\"Get a dataframe for a single sweep.\"\"\"\n",
                "\n",
                "    def create_run_df(history, config):\n",
                "        for k, v in config.items():\n",
                "            if k == \"momentum\" and isinstance(v, list):\n",
                "                v = [tuple(v)] * len(history)\n",
                "            history[k] = v\n",
                "\n",
                "        return history\n",
                "\n",
                "    return pd.concat([create_run_df(run.history(keys=KEYS), run.config) for run in runs])\n",
                "\n",
                "\n",
                "def combine_histories(\n",
                "    histories,\n",
                "    unique_cols: Union[List[str], str] = \"weight_decay\",\n",
                "    allow_duplicates: bool = False,\n",
                "):\n",
                "    \"\"\"\n",
                "    Gathers all the runs from a series of sweeps and combines them into a single dataframe.\n",
                "\n",
                "    `unique_col` is used to identify duplicate runs. By default, `\"_step\"` is added.\n",
                "    If there are duplicates, the run from the last sweep is kept.\n",
                "    \"\"\"\n",
                "    unique_cols = unique_cols if isinstance(unique_cols, list) else [unique_cols]\n",
                "    histories = pd.concat(histories)\n",
                "\n",
                "    # Change column names\n",
                "    histories = histories.rename(columns={\"train_step\": \"_step\", \"Accuracy/train\": \"train/acc\", \"Accuracy/val\": \"test/acc\", \"Loss/val\": \"train/loss\", \"Loss/test\": \"test/loss\", \"Loss/train/avg\": \"train/loss/avg\", \"Loss/test/avg\": \"test/loss/avg\"})\n",
                "\n",
                "    if not allow_duplicates:\n",
                "        histories = histories.drop_duplicates([\"_step\", *unique_cols], keep=\"last\")\n",
                "\n",
                "    # Change step 0 to 1 to avoid issues with log plots\n",
                "    histories.loc[histories._step == 0, \"_step\"] = 1\n",
                "\n",
                "    # Fix types\n",
                "    histories.applymap(lambda x: x.item() if isinstance(x, np.generic) else x)\n",
                "    non_numeric_columns = histories.select_dtypes(\n",
                "        exclude=[\"int\", \"float\", \"int64\", \"float64\"]\n",
                "    ).columns\n",
                "    histories = histories.drop(columns=non_numeric_columns)\n",
                "\n",
                "    # Sort\n",
                "    histories = histories.sort_values(by=[*unique_cols, \"_step\"])\n",
                "\n",
                "    return histories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "histories = []\n",
                "START = 0\n",
                "\n",
                "for run in tqdm(accepted[START:]):\n",
                "    print(run)\n",
                "    histories.append(run.history())\n",
                "    \n",
                "# combine_histories(histories, unique_cols=[\"d_model\"])\n",
                "histories[0].columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "KEYS = [\"Accuracy/train\", \"Accuracy/val\", \"Loss/train\", \"Loss/val\", \"train_step\"]\n",
                "\n",
                "# Drop all other columns\n",
                "histories_trimmed = [h[KEYS] for h in histories]\n",
                "histories_trimmed[0]\n",
                "\n",
                "# Rename columns\n",
                "histories_trimmed = [h.rename(columns={\"train_step\": \"_step\", \"Accuracy/train\": \"train/acc\", \"Accuracy/val\": \"test/acc\", \"Loss/train\": \"train/loss\", \"Loss/val\": \"test/loss\",}) for h in histories_trimmed]\n",
                "for i, h in enumerate(histories_trimmed):\n",
                "    h[\"d_model\"] = accepted[i].config[\"d_model\"]\n",
                "\n",
                "histories_combined = pd.concat(histories_trimmed)\n",
                "\n",
                "# Order by d_model, then step\n",
                "histories_combined = histories_combined.sort_values(by=[\"d_model\", \"_step\"])\n",
                "\n",
                "# Save to csv\n",
                "histories_combined.to_csv(\"logs/mw_sweep.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unifying.sweep import METRICS, get_pivot\n",
                "from scipy.ndimage import gaussian_filter\n",
                "\n",
                "def get_mw_sweep(metrics=[\"test/acc\", \"train/acc\", \"test/loss\", \"train/loss\"], steps=None, num_steps=2000):\n",
                "    df = pd.read_csv(\"../logs/mw_sweep.csv\")\n",
                "    return df\n",
                "    MINIMUM = 1/97\n",
                "    missing_row = df.loc[df._step == 60, :].copy()\n",
                "    missing_row._step = 1\n",
                "    missing_row[\"train/acc\"] = MINIMUM\n",
                "    missing_row[\"test/acc\"] = MINIMUM\n",
                "    df = pd.concat(\n",
                "        [\n",
                "            missing_row,\n",
                "            df,\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    d_models = sorted(df[\"d_model\"].unique())\n",
                "    print(d_models)\n",
                "    df = get_pivot(df, \"d_model\", metrics, reindex=True, interpolate=True) \n",
                "\n",
                "    steps = steps or list({int(s) for s in np.logspace(0, np.log10(df.index.max()), num_steps)})\n",
                "    df = df.loc[df.index.isin(steps), :]\n",
                "\n",
                "    df = df.reset_index()  # to make _step a regular column\n",
                "    df.columns = ['_step'] + [f'{x}_{y}' for x, y in df.columns[1:]]  # to make columns single level\n",
                "    df_melted = df.melt(id_vars='_step', var_name='variable', value_name='value')\n",
                "\n",
                "    # split the variable column into the original columns\n",
                "    df_melted[['metric', 'd_model']] = df_melted['variable'].str.split('_', expand=True)\n",
                "    df_melted['d_model'] = df_melted['d_model'].astype(int)  # convert d_model back to integer\n",
                "\n",
                "    # pivot to get the original columns\n",
                "    df_final = df_melted.pivot(index=['_step', 'd_model'], columns='metric', values='value').reset_index()\n",
                "\n",
                "    return df_final\n",
                "\n",
                "histories_combined = get_mw_sweep()\n",
                "d_model_vals = histories_combined.d_model.unique()\n",
                "print(d_model_vals)\n",
                "histories_combined\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model-wise Details\n",
                "D_MODEL = 128\n",
                "# Omit all runs with d_model < 30\n",
                "mw = histories_combined[histories_combined.d_model >= 10]\n",
                "STEP = histories_combined._step.unique()[-10]\n",
                "plot_all_details(mw, title=\"Embedding dim.\", unique_col=\"d_model\", run_val=D_MODEL, plot_extra=False, smooth=(3, 0.5), step=STEP, cmap=\"magma\", log_x=False, log_time=True, flip=False, log_loss=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample-wise Details\n",
                "\n",
                "# plot_curves_2x2(sw_grokking_cleaned, title=\"Fraction of Training Data\", unique_col=\"frac_train\")\n",
                "\n",
                "# plot_slice(sw_grokking_cleaned, 10000, log_loss=False, titles = {\n",
                "#     \"train/loss\": \"Train Loss\",\n",
                "#     \"test/loss\": \"Validation Loss\",\n",
                "#     \"train/acc\": \"Train Accuracy\",\n",
                "#     \"test/acc\": \"Validation Accuracy\"\n",
                "# }, title=\"Fraction of Training Data\", unique_col=\"frac_train\", log_x=False)\n",
                "\n",
                "# plot(sw_grokking_cleaned, smooth=0.25, log_loss=True, titles = {\n",
                "#     \"train/loss\": \"Train Loss\",\n",
                "#     \"test/loss\": \"Validation Loss\",\n",
                "#     \"train/acc\": \"Train Accuracy\",\n",
                "#     \"test/acc\": \"Validation Accuracy\"\n",
                "# }, unique_col=\"frac_train\", title=\"Training Fraction\", log_x=False, log_y=False)\n",
                "\n",
                "frac_trains = sw_grokking_cleaned.frac_train.unique()\n",
                "frac_train_selection = frac_trains[6] \n",
                "\n",
                "frac_train_bads = frac_trains[-5:]\n",
                "# Delete the bad run\n",
                "sw_grokking_cleaned_2 = sw_grokking_cleaned[~sw_grokking_cleaned.frac_train.isin(frac_train_bads)]\n",
                "\n",
                "plot_all_details(sw_grokking_cleaned_2, title=\"Training fraction\", unique_col=\"frac_train\", run_val=frac_train_selection, plot_extra=False, cmap=\"magma\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Regularization-wises Details\n",
                "\n",
                "# plot_curves_2x2(rw_grokking_cleaned, title=\"Weight Decay\", unique_col=\"weight_decay\")\n",
                "# plot_slice(rw_grokking_cleaned, 10010, smooth=True, log_loss=False, titles = {\n",
                "#     \"train/loss\": \"Train Loss\",\n",
                "#     \"test/loss\": \"Validation Loss\",\n",
                "#     \"train/acc\": \"Train Accuracy\",\n",
                "#     \"test/acc\": \"Validation Accuracy\"\n",
                "# }, title=\"Weight Decay\")\n",
                "\n",
                "# plot(rw_grokking_cleaned,   smooth=True, log_loss=False, titles = {\n",
                "#     \"train/loss\": \"Train Loss\",\n",
                "#     \"test/loss\": \"Validation Loss\",\n",
                "#     \"train/acc\": \"Train Accuracy\",\n",
                "#     \"test/acc\": \"Validation Accuracy\"\n",
                "# }, title=\"Weight Decay\")\n",
                "\n",
                "\n",
                "plot_all_details(rw_grokking_cleaned, title=\"Weight Decay\", unique_col=\"weight_decay\", run_val=wd_selection, plot_extra=False, cmap=\"magma\", smooth=2., log_loss=True, step=20_000)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Grid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pivot_table_train = get_pivot(histories_combined, \"d_model\", columns=[\"train/acc\"], reindex=True, interpolate=True)\n",
                "pivot_table_test = get_pivot(histories_combined, \"d_model\", columns=[\"test/acc\"], reindex=True, interpolate=True)\n",
                "STEP = 40000\n",
                "# Get the closest _step to TARGET_STEP\n",
                "# STEP = pivot_table._step.iloc[(pivot_table._step - TARGET_STEP).abs().argsort()[:1]].values[0]\n",
                "# print(STEP)\n",
                "mw_slice_train_acc = extract_slice_from_pivot(pivot_table_train, STEP, \"train/acc\", \"d_model\", smooth=(10, 4))\n",
                "mw_slice_test_acc = extract_slice_from_pivot(pivot_table_test, STEP, \"test/acc\", \"d_model\", smooth=(10, 4))\n",
                "\n",
                "mw_slice = pd.DataFrame({\n",
                "    \"d_model\": mw_slice_train_acc.index,\n",
                "    \"train/acc\": mw_slice_train_acc[\"train/acc\"].values,\n",
                "    \"test/acc\": mw_slice_test_acc[\"test/acc\"].values,\n",
                "})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mw_slice.plot(x=\"d_model\", y=[\"train/acc\", \"test/acc\"], logx=False, logy=False, figsize=(10, 5), title=\"MW Sweep\", xlabel=\"d_model\", ylabel=\"Accuracy\", legend=True, color=[\"C0\", \"C1\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the 1x4 grid\n",
                "\n",
                "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 4))\n",
                "\n",
                "titles = [\"Epoch-wise\", \"Model-wise\", \"Sample-wise\", \"Regularization-wise\"]\n",
                "xlabels = [\"Steps\", \"Embedding Dim.\", \"Fraction of Training Data\", \"Weight Decay\"]\n",
                "ylabels = [\"Accuracy\", \"\", \"\", \"\"]\n",
                "data = [(steps, ew_slice), (d_model_vals, mw_slice), (frac_train_vals, sw_slice), (wd_vals, rw_slice)]\n",
                "\n",
                "for ax, title, xlabel, ylabel, (x, y) in zip(axes, titles, xlabels, ylabels, data):\n",
                "\n",
                "    ax.set_ylim(0., 1.05)\n",
                "    ax.set_xlabel(xlabel)\n",
                "    ax.set_ylabel(ylabel)\n",
                "\n",
                "    ax.plot(x, y[\"train/acc\"], label=\"Train\", color=BLUE, linewidth=3.5)\n",
                "    ax.plot(x, y[\"test/acc\"], label=\"Test\", color=RED, linewidth=3.5)\n",
                "\n",
                "    # Increase the font size of the x and y labels\n",
                "    ax.xaxis.label.set_size(24)\n",
                "    ax.yaxis.label.set_size(24)\n",
                "\n",
                "\n",
                "axes[0].set_xscale('log')\n",
                "axes[1].set_xscale('log')\n",
                "axes[2].set_xscale('log')\n",
                "axes[3].set_xscale('log')\n",
                "\n",
                "# Add a legend at the bottom of all the plots (to be shared)\n",
                "# axes[0].legend(loc='upper center', bbox_to_anchor=(2.25, -0.3), fancybox=False, shadow=False, ncol=2)\n",
                "# axes[3].legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), fancybox=False, shadow=False, ncol=2)\n",
                "axes[3].legend(fontsize=20, loc='lower right', fancybox=False, shadow=False, ncol=1)\n",
                "\n",
                "fig.savefig(\"../figures/grokking_extensions.pdf\", bbox_inches='tight')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Interpolations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def plot_interpolation_overview(\n",
                "    df: pd.DataFrame,\n",
                "    unique_col: str = \"weight_decay\",\n",
                "    smooth: Union[bool, float] = False,\n",
                "    cmap=\"inferno\",\n",
                "    log_loss=True,\n",
                "    log_x: bool = True,\n",
                "    log_y: bool = True,\n",
                "    title: str = \"\",\n",
                "    metric: str = \"test/acc\",\n",
                "    metric_label: str = \"Accuracy\",\n",
                "    run_vals: List[float] = [0.0],\n",
                "    plot_extra: bool = False,\n",
                "    latex: str = \"\",\n",
                "    suptitle: str = \"\",\n",
                "):\n",
                "    metric_label_short = (\n",
                "        metric_label.split(\" \")[1] if \" \" in metric_label else metric_label\n",
                "    )\n",
                "\n",
                "    num_snapshots = len(run_vals)\n",
                "\n",
                "    # create a figure with a 2x2 grid of subplots\n",
                "    fig = plt.figure(figsize=(10, 6))\n",
                "    gs = gridspec.GridSpec(num_snapshots, 2, width_ratios=[3, 2], hspace=0.25)\n",
                "\n",
                "    pivot_table = get_pivot(\n",
                "        df, unique_col, reindex=True, interpolate=True, columns=[metric]\n",
                "    )\n",
                "    unique_vals = sorted(df[unique_col].unique())\n",
                "\n",
                "    ax1 = plt.subplot(gs[:, 0])\n",
                "    y = pivot_table[metric].index\n",
                "    mesh = create_heatmap(\n",
                "        x=unique_vals,\n",
                "        y=y,\n",
                "        z=pivot_table[metric].values,\n",
                "        ax=ax1,\n",
                "        smooth=smooth,\n",
                "        cmap=cmap,\n",
                "        log_x=log_x,\n",
                "        log_y=log_y,\n",
                "        log_z=log_loss and \"loss\" in metric,\n",
                "        title=title,\n",
                "    )\n",
                "\n",
                "    fig.colorbar(mesh, ax=ax1, label=\"Test Accuracy\")\n",
                "\n",
                "    # Plot horizontal lines at the run_vals entries\n",
                "    run_vals_lines = [(v + 0.0125 if v == 0 else v) for v in run_vals]\n",
                "    ax1.vlines(run_vals_lines, y.min(), y.max(), color=RED)\n",
                "\n",
                "    # Find the _step for each run where the train/acc first reaches 1.0\n",
                "    interpolation = []\n",
                "    convergence = []\n",
                "\n",
                "    for i, val in enumerate(unique_vals):\n",
                "        run = df.loc[(df[unique_col] == val), :]\n",
                "\n",
                "        interp_max = run[\"train/acc\"].max()\n",
                "        interp_threshold = interp_max * 0.95\n",
                "        interp_step = run.loc[run[\"train/acc\"] > interp_threshold, \"_step\"].min()  # type: ignore\n",
                "        interp_val = run.loc[run._step == interp_step, \"train/acc\"].values[0]  # type: ignore\n",
                "        interpolation.append((interp_step, interp_val))\n",
                "\n",
                "        conv_max = run[metric].max()\n",
                "        conv_threshold = conv_max * 0.95\n",
                "        conv_step = run.loc[run[metric] > conv_threshold, \"_step\"].min()  # type: ignore\n",
                "        conv_val = run.loc[run._step == conv_step, metric].values[0]  # type: ignore\n",
                "        convergence.append((conv_step, conv_val))\n",
                "\n",
                "    ax1.plot(\n",
                "        unique_vals,\n",
                "        [v for (v, _) in interpolation],\n",
                "        color=\"grey\",\n",
                "        linestyle=\"--\",\n",
                "        label=\"Interpolation\",\n",
                "    )\n",
                "    ax1.plot(\n",
                "        unique_vals,\n",
                "        [v for (v, _) in convergence],\n",
                "        color=\"grey\",\n",
                "        linestyle=\"--\",\n",
                "        label=\"Convergence\",\n",
                "    )\n",
                "\n",
                "    for i, val in enumerate(run_vals):\n",
                "        run = df.loc[(df[unique_col] == val), :]\n",
                "        ax = plt.subplot(gs[i, 1])\n",
                "\n",
                "        # Interpolation threshold\n",
                "        unique_val_idx = unique_vals.index(val)\n",
                "\n",
                "        interp_step, interp_val = interpolation[unique_val_idx]\n",
                "        ax.vlines(interp_step, 0, interp_val, color=\"grey\", linestyle=\"--\", label=None)\n",
                "\n",
                "        # Test convergence threshold\n",
                "        conv_step, conv_val = convergence[unique_val_idx]\n",
                "        ax.vlines(conv_step, 0, conv_val, color=\"grey\", linestyle=\"--\", label=None)\n",
                "\n",
                "        ax.plot(run._step, run[\"train/acc\"], color=BLUE, label=\"Train\")\n",
                "        ax.plot(run._step, run[metric], color=RED, label=\"Test\")\n",
                "\n",
                "        val_rounded = round(val, 2)\n",
                "        ax.set_title(f\"{metric_label_short} for ${latex}={val_rounded}$\")\n",
                "\n",
                "        ax.set_ylim([0.0, 1.05])\n",
                "        ax.set_xscale(\"log\")\n",
                "        ax.set_xlim([1e0, df._step.max()])\n",
                "\n",
                "        if i < len(run_vals) - 1:\n",
                "            ax.set_xticklabels([])\n",
                "        else:\n",
                "            ax.set_xlabel(\"Step\")\n",
                "\n",
                "        if i == 0:\n",
                "            ax.legend(loc=\"lower right\")\n",
                "\n",
                "    plt.suptitle(suptitle)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ln_sweep = get_history(\"xwna3rhg\", allow_duplicates=True)\n",
                "\n",
                "# ln_sweep_cleaned = handle_outliers(\n",
                "#     ln_sweep,\n",
                "#     loss_cols=[\"train/loss\"], \n",
                "#     action=\"keep\",\n",
                "#     unique_cols=[\"frac_train\"],\n",
                "#     threshold=0.0001,\n",
                "#     late_epochs_ratio=0.6,\n",
                "# )\n",
                "\n",
                "\n",
                "def correct_label_noise(df: pd.DataFrame):\n",
                "    # Correct the sweeps by dividing test/acc by 1-label_noise\n",
                "    df = df.copy()\n",
                "\n",
                "    lbl_noises = df.frac_label_noise.unique()\n",
                "\n",
                "    for lbl_noise in lbl_noises:\n",
                "        for metric in [\"test/acc\"]:\n",
                "            df.loc[df.frac_label_noise==lbl_noise, metric] /= 1 - lbl_noise\n",
                "\n",
                "    return df\n",
                "\n",
                "ln_sweep_cleaned = correct_label_noise(ln_sweep)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sns.lineplot(ln_sweep, x=\"_step\", y=\"test/acc\", hue=\"frac_label_noise\", legend=False, palette=\"coolwarm\", )\n",
                "plt.xscale('log')\n",
                "plt.xlabel(\"Steps\")\n",
                "plt.show()\n",
                "\n",
                "sns.lineplot(ln_sweep_cleaned, x=\"_step\", y=\"test/acc\", hue=\"frac_label_noise\", legend=False, palette=\"coolwarm\", )\n",
                "plt.xscale('log')\n",
                "plt.xlabel(\"Steps\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_interpolation_overview(\n",
                "    ln_sweep_cleaned,\n",
                "    unique_col=\"frac_label_noise\",  \n",
                "    title=\"Fraction training noise\",\n",
                "    cmap=\"coolwarm\",\n",
                "    run_vals=[0.0, 0.05, 0.15],\n",
                "    latex=r\"f_\\mathrm{noise}\",\n",
                "    log_x=False,\n",
                "    suptitle=\"Modular Arithmetic\"\n",
                ")   "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "frac_train_sweep = get_history(\"cnr2n43n\", \"759uhy5f\", allow_duplicates=True, project=\"mnist-grokking\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ft_vals = sorted(frac_train_sweep.frac_train.unique())\n",
                "ft_selection = [ft_vals[-3], ft_vals[8], ft_vals[1]]\n",
                "\n",
                "plot_interpolation_overview(\n",
                "    frac_train_sweep,\n",
                "    unique_col=\"frac_train\",\n",
                "    title=\"Training Fraction\",\n",
                "    cmap=\"coolwarm\",\n",
                "    run_vals=ft_selection,\n",
                "    latex=r\"f_\\mathrm{train}\",\n",
                "    log_x=True,\n",
                "    suptitle=\"MNIST\"\n",
                ") "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "api = wandb.Api()\n",
                "mnist_dd = api.run(f\"{ENTITY}/mnist-grokking/hbw3nlnb\")\n",
                "mnist_grok = api.run(f\"{ENTITY}/mnist-grokking/9i790lal\")\n",
                "\n",
                "mnist_dd_df = pd.DataFrame(mnist_dd.history())\n",
                "mnist_grok_df = pd.DataFrame(mnist_grok.history())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2 by 2 grid of subplots\n",
                "# Acc on left, loss on right\n",
                "# DD on top, Grokking on bottom\n",
                "# Train in blue, test in red\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
                "\n",
                "for i, (df, title) in enumerate(zip([mnist_dd_df, mnist_grok_df], [\"DD\", \"Grokking\"])):\n",
                "    ax = axes[i, 0]\n",
                "    # Sort by steps\n",
                "    df = df.sort_values(\"_step\")\n",
                "\n",
                "    ax.plot(df._step, df[\"train/acc\"], color=BLUE, label=\"Train\")\n",
                "    ax.plot(df._step, df[\"test/acc\"], color=RED, label=\"Test\")\n",
                "    ax.set_title(f\"{title} Accuracy\")\n",
                "    ax.set_ylim([0., 1.05])\n",
                "    ax.set_xscale(\"log\")\n",
                "    ax.set_xlim([1e0, df._step.max()])\n",
                "    ax.legend(loc=\"lower right\")\n",
                "\n",
                "    ax = axes[i, 1]\n",
                "    ax.plot(df._step, df[\"train/loss\"], color=BLUE, label=\"Train\")\n",
                "    ax.plot(df._step, df[\"test/loss\"], color=RED, label=\"Test\")\n",
                "    ax.set_title(f\"{title} Loss\")\n",
                "    ax.set_xscale(\"log\")\n",
                "    ax.set_yscale(\"log\")\n",
                "    ax.set_xlim([1e0, df._step.max()])\n",
                "    ax.legend(loc=\"upper right\")\n",
                "\n",
                "# More space between rows\n",
                "fig.tight_layout(pad=1.5)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Miscellaneous\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Can we induce epoch-/regularization-wise DD in shallow models?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Can we induce epoch-wise DD in transformers?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "\n",
                "# Define an empty dataset\n",
                "class EmptyDataset(Dataset):\n",
                "    def __init__(self):\n",
                "        pass\n",
                "\n",
                "    def __len__(self):\n",
                "        return 0\n",
                "\n",
                "    def __getitem__(self, index):\n",
                "        pass\n",
                "\n",
                "# Create an empty dataset object\n",
                "empty_dataset = EmptyDataset()\n",
                "\n",
                "# Create an empty dataloader\n",
                "dataloader = DataLoader(empty_dataset, batch_size=32, shuffle=True)\n",
                "\n",
                "# Iterate over the dataloader (won't execute any iterations since the dataset is empty)\n",
                "for batch in dataloader:\n",
                "    print(batch)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}