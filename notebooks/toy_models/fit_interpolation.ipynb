{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install numpy torch sympy mod blobfile pandas seaborn matplotlib tqdm einops wandb\n",
                "\n",
                "import sys\n",
                "import os\n",
                "\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
                "\n",
                "from contextlib import suppress\n",
                "from dataclasses import dataclass, asdict\n",
                "from datetime import datetime\n",
                "from typing import Callable, Literal, Optional, Union, Tuple, List\n",
                "from copy import deepcopy\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "from torch import nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader\n",
                "from torch import optim\n",
                "import wandb\n",
                "import os

if os.getenv('USE_TQDM_NOTEBOOK', 'NO').lower() in ['yes', 'true', '1'
                ]:
    from tqdm.notebook import tqdm
else:
    from tqdm import tqdm
\n","import ipywidgets as widgets\n","import wandb\n",
                "\n",
                "import matplotlib as mpl\n",
                "from matplotlib.colors import LogNorm\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from patterns.dataset import ModularArithmetic, Operator\n",
                "from patterns.transformer import Transformer\n",
                "from patterns.utils import generate_run_name\n",
                "from patterns.learner import Config\n",
                "\n",
                "from toy_models.fit import rescale_run, Pattern, PatternLearningModel\n",
                "from unifying.sweep import get_history, handle_outliers\n",
                "from unifying.plotting import BLUE, RED\n",
                "\n",
                "DEFAULT_MODULUS = 113\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "sns.set_theme(style=\"darkgrid\")\n",
                "\n",
                "ENTITY = \"<Insert WANDB entity>\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_patterns(pl_model, run, log=False):\n",
                "    ts = run[\"_step\"].values\n",
                "    train_preds = [pl_model(t).detach().numpy() for t in ts]\n",
                "    test_preds = [pl_model.test(t).detach().numpy() for t in ts]\n",
                "    train_ys = torch.tensor(run[\"train/acc\"].values).float()\n",
                "    test_ys = torch.tensor(run[\"test/acc\"].values).float()\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
                "\n",
                "    axes[0].plot(ts, train_preds, label=\"train\", color=\"blue\")\n",
                "    axes[0].plot(ts, test_preds, label=\"test\", color=\"red\")\n",
                "\n",
                "    axes[1].plot(ts, train_ys, label=\"train\", color=\"blue\")\n",
                "    axes[1].plot(ts, test_ys, label=\"test\", color=\"red\")\n",
                "\n",
                "    axes[0].set_title(\"Predictions\")\n",
                "    axes[1].set_title(\"True values\")\n",
                "\n",
                "    if log:\n",
                "        axes[0].set_xscale(\"log\")\n",
                "        axes[1].set_xscale(\"log\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unifying.sweep import get_pivot, METRICS\n",
                "\n",
                "INTERP_SWEEPS = [\"kodd01ka\", \"wecya83q\", \"wqnakkjd\"] #, \"awxzpem1\"]\n",
                "interp_sweep = get_history(*INTERP_SWEEPS, project=\"mnist-grokking\", allow_duplicates=True, combine_seeds=True)\n",
                "# # interp_sweep.drop([\"weight/cos_sim_with_init\", \"test/efficiency\", \"train/efficiency\", \"weight/dist_from_init\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "interp_sweep_og = interp_sweep.copy()\n",
                "interp_sweep.loc[:, METRICS]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "unique_cols = [\"lr_factor\"]\n",
                "histories = interp_sweep\n",
                "metrics = METRICS\n",
                "\n",
                "assert (\n",
                "    len(unique_cols) == 1\n",
                "), \"Can only combine seeds if there is a single unique column\"\n",
                "\n",
                "unique_col = unique_cols[0]\n",
                "unique_vals = histories[unique_col].unique()\n",
                "\n",
                "for val in unique_vals:\n",
                "    runs = histories[histories[unique_col] == val]\n",
                "    seeds = runs.seed.unique()\n",
                "\n",
                "    if len(seeds) > 1:\n",
                "        # Define the metrics that need to be averaged\n",
                "        for metric in metrics:\n",
                "            # Calculate the mean value for each metric and _step\n",
                "            means_groups = runs.groupby(\"_step\")[metric]\n",
                "\n",
                "            means = means_groups.apply(\n",
                "                lambda x: x.ffill().bfill().mean() if x.isna().any() else x.mean()\n",
                "            )\n",
                "\n",
                "            # Update the histories dataframe\n",
                "            for _step, mean_value in means.items():\n",
                "                mask = (histories[unique_col] == val) & (\n",
                "                    histories._step == _step\n",
                "                )\n",
                "                histories.loc[mask, metric] = mean_value\n",
                "\n",
                "# Remove duplicate rows\n",
                "histories = histories.drop_duplicates(subset=[*unique_cols, \"_step\"])\n",
                "histories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "interp_sweep = histories\n",
                "\n",
                "# Drop _step == 0\n",
                "interp_sweep = interp_sweep[interp_sweep._step != 0]\n",
                "\n",
                "# Set \"corrupted/acc\" etc. = 0.1 at _step == 1\n",
                "# for m in [\"corrupted/acc\", \"uncorrupted/acc\", \"test/acc\", \"train/acc\"]:\n",
                "#   interp_sweep.loc[interp_sweep._step == 1, m] = 0.1\n",
                "\n",
                "interp_sweep[interp_sweep[\"corrupted/acc\"].notna()]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.ndimage import gaussian_filter1d, gaussian_filter\n",
                "\n",
                "# Get pivot_table\n",
                "df = get_pivot(interp_sweep, \"lr_factor\", METRICS, reindex=True, interpolate=True) \n",
                "lr_factors = sorted(interp_sweep[\"lr_factor\"].unique())\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.ndimage import gaussian_filter1d, gaussian_filter\n",
                "\n",
                "df_smoothed = df.copy()\n",
                "\n",
                "for m in METRICS:\n",
                "    unique_vals = [c[1] for c in df.columns if c[0] == m]\n",
                "    _df = gaussian_filter(df[m], sigma=(10., 1.5))\n",
                "\n",
                "    for i, unique_val in enumerate(unique_vals):\n",
                "        df_smoothed[(m, unique_val)] = _df[:, i]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_smoothed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# df = df_filtered.reindex(np.arange(df_filtered._step.min(), df_filtered._step.max() + 1))\n",
                "# df = df.interpolate(method=\"linear\", axis=0, inplace=True).fillna(method=\"bfill\")\n",
                "LR_FACTOR = 5.625\n",
                "run = df[[col for col in df.columns if col[1] == LR_FACTOR]]\n",
                "# run = interp_sweep.loc[interp_sweep.lr_factor == LR_FACTOR, :]\n",
                "# steps = sorted(run.index.unique())\n",
                "# run = run[steps, :]\n",
                "# Convert back to non-pivot\n",
                "run = pd.DataFrame(run.stack().reset_index())\n",
                "run"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "histories_combined = pd.read_csv(\"../logs/mw_sweep.csv\")\n",
                "# histories_combined.dropna(inplace=True)\n",
                "histories_combined.interpolate(method=\"linear\", axis=0, inplace=True)\n",
                "\n",
                "d_model_vals = histories_combined.d_model.unique()\n",
                "print(d_model_vals)\n",
                "D_MODEL = 128\n",
                "run = histories_combined.loc[histories_combined.d_model == D_MODEL, :]\n",
                "run"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(run._step, run[\"train/acc\"], label=\"train\")\n",
                "# plt.plot(run.index, run[\"train/acc\"], label=\"smoothed\")\n",
                "plt.plot(run._step, run[\"test/acc\"], label=\"test\")\n",
                "# plt.plot(run.index, run[\"test/acc\"], label=\"smoothed\")\n",
                "plt.xscale(\"log\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(run._step, run[\"corrupted/acc\"], label=\"corrupted\")\n",
                "# plt.plot(run.index, run[\"train/acc\"], label=\"smoothed\")\n",
                "plt.plot(run._step, run[\"uncorrupted/acc\"], label=\"uncorrupted\")\n",
                "# plt.plot(run.index, run[\"test/acc\"], label=\"smoothed\")\n",
                "plt.xscale(\"log\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.ndimage import gaussian_filter1d\n",
                "\n",
                "steps = run._step.unique()\n",
                "og_steps = interp_sweep.loc[interp_sweep.lr_factor == LR_FACTOR]._step.unique()\n",
                "og_steps = np.array([s - 1 for s in og_steps])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_sample = run.loc[og_steps, :]\n",
                "rescaled_run = rescale_run(run_sample, new_max=100., log=False) \n",
                "\n",
                "# for metric in METRICS:\n",
                "#     rescaled_run.loc[metric, :] = gaussian_filter1d(rescaled_run.loc[metric,:])\n",
                "\n",
                "rescaled_run.plot(x=\"_step\", y=[\"train/acc\", \"test/acc\"], logx=True, figsize=(10, 5))\n",
                "rescaled_run"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = PatternLearningModel(max_time=100.)\n",
                "\n",
                "# Initialization\n",
                "for i, pattern in enumerate(model.patterns):\n",
                "    max_time = 100\n",
                "    pattern.onset.data = torch.tensor(0.1 * (0.25 * max_time) ** i)\n",
                "    pattern.speed.data = torch.tensor((max_time / 2) * 10 ** (-i))\n",
                "    # pattern._strength.data = pattern._inv_sigmoid(torch.tensor([.8, 1.0, 1.0][i]))\n",
                "    # pattern._generalization.data = torch.log(torch.tensor([.3, 0.01, .69][i]))\n",
                "\n",
                "print(model.patterns)\n",
                "\n",
                "def callback(x): \n",
                "    plot_patterns(x, rescaled_run, log=True)\n",
                "    plt.show()\n",
                "\n",
                "callback(model)\n",
                "\n",
                "model.fit(rescaled_run, lr=0.1, num_epochs=500, callback=callback, callback_ivl=25)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.fit(rescaled_run, lr=0.01, num_epochs=400, callback=callback, callback_ivl=25)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(model.patterns)\n",
                "model.fit(rescaled_run, lr=0.01, num_epochs=1000, callback=callback, callback_ivl=50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "with open(\"dd_fit.pkl\", \"wb\") as f:\n",
                "    pickle.dump(model, f)\n",
                "\n",
                "model.patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Investigate how local the minimum is\n",
                "\n",
                "model.patterns[1].onset.data *= 2\n",
                "model.patterns[1].speed.data /= 2."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.fit(rescaled_run, lr=0.01, num_epochs=500, callback=callback, callback_ivl=50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "with open(\"dd_fit.pkl\", \"wb\") as f:\n",
                "    pickle.dump(model, f)\n",
                "\n",
                "model.patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.rescale(500_000) \n",
                "model.patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
                "\n",
                "ax.plot(run[\"_step\"], run[\"test/acc\"], label=\"Test (Truth)\", color=RED, linewidth=2, alpha=0.5)\n",
                "ax.plot(run[\"_step\"], run[\"train/acc\"], label=\"Train (Truth)\", color=BLUE, linewidth=2, alpha=0.5)\n",
                "ax.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "# ax.set_xlabel(\"Steps\", fontsize=18)\n",
                "# ax.set_xticklabels([\"\", \"\", \"\", \"\", \"\", \"\"], color=\"white\")\n",
                "\n",
                "min_, max_ = 0, 100\n",
                "ts = torch.linspace(min_, max_, 10000)\n",
                "TS = ts * 500_000 / 100\n",
                "train_ys = [model(t).detach().numpy() for t in ts]\n",
                "test_ys = [model.test(t).detach().numpy() for t in ts]\n",
                "ax.plot(TS, train_ys, label=\"Train (Fit)\", color=BLUE, linewidth=2, linestyle=\"--\")\n",
                "ax.plot(TS, test_ys, label=\"Test (Fit)\", color=RED, linewidth=2, linestyle=\"--\")\n",
                "# ax.set_title(\"Fit\", )\n",
                "ax.legend(fontsize=14)\n",
                "\n",
                "ax.set_xticklabels([\"$10^0$\", \"$10^1$\", \"$10^2$\", \"$10^3$\", \"$10^4$\", \"$10^5$\"])\n",
                "# ax.set_xlim(10, 100)\n",
                "ax.set_xscale(\"log\")\n",
                "# ax.set_xticks([0.01, 0.1, 1, 10, 100])\n",
                "\n",
                "fig.tight_layout(pad=0.25)\n",
                "ax.set_xlabel(\"Steps\", fontsize=18)\n",
                "\n",
                "\n",
                "# Already in log scale\n",
                "# train_ys, test_ys\n",
                "\n",
                "plt.savefig(\"../figures/dd-fit.pdf\", bbox_inches=\"tight\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fit the sweeps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "api = wandb.Api()\n",
                "\n",
                "entity = \"<TODO: FILL IN>\"\n",
                "runs = api.runs(f\"{entity}/fit-toy-model\")\n",
                "[run for run in runs]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mw_fit_run_id = \"1gthqq5\"\n",
                "run = runs[2]\n",
                "df = run.history()\n",
                "\n",
                "col = \"d_model\"\n",
                "unique_vals = df.loc[:, col].unique()\n",
                "print(unique_vals)\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def df_row_to_toy_model(row):\n",
                "    model = PatternLearningModel(max_time=1.)\n",
                "\n",
                "    for i, pattern in enumerate(model.patterns):\n",
                "        pattern.onset.data = torch.tensor(row[f\"pattern_{i}/onset\"])\n",
                "        pattern.speed.data = torch.tensor(row[f\"pattern_{i}/speed\"])\n",
                "        pattern._strength.data = pattern._inv_sigmoid(torch.tensor(row[f\"pattern_{i}/strength\"]))  # type: ignore\n",
                "        pattern._generalization.data = torch.log(torch.tensor(row[f\"pattern_{i}/generalization\"]))\n",
                "\n",
                "    return model\n",
                "\n",
                "D_MODEL = 115\n",
                "co9l = \"d_model\"\n",
                "model_entry = df.loc[df[col] == D_MODEL, :].iloc[0, :]\n",
                "print(model_entry)\n",
                "model = df_row_to_toy_model(model_entry)\n",
                "model.rescale(100)\n",
                "model.patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get corresponding original run\n",
                "og_df = get_history(DM_SWEEP_ID, unique_cols=\"d_model\")\n",
                "run = og_df.loc[og_df.d_model==D_MODEL,:] #.plot(x=\"_step\", y=\"test/acc\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))\n",
                "\n",
                "ax1.plot(run[\"_step\"], run[\"test/acc\"], label=\"Test\", color=RED, linewidth=2)\n",
                "ax1.plot(run[\"_step\"], run[\"train/acc\"], label=\"Train\", color=BLUE, linewidth=2)\n",
                "ax1.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "# ax1.set_xlabel(\"Steps\", fontsize=18)\n",
                "ax1.set_xticklabels([\"\", \"\", \"\", \"\", \"\", \"\"], color=\"white\")\n",
                "ax1.set_xscale(\"log\")\n",
                "ax1.legend(title=\"Truth\", fontsize=16, title_fontsize=18)\n",
                "\n",
                "min_step, max_step = og_df[\"_step\"].min(), 100 # run[\"_step\"].max()\n",
                "\n",
                "ts = np.linspace(12, max_step, 1000)\n",
                "train_ys = [model(t).detach().numpy() for t in ts]\n",
                "test_ys = [model.test(t).detach().numpy() for t in ts]\n",
                "ax2.plot(ts, train_ys, label=\"Train\", color=BLUE, linewidth=2)\n",
                "ax2.plot(ts, test_ys, label=\"Test\", color=RED, linewidth=2)\n",
                "ax2.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "ax2.set_xlabel(\"Steps\", fontsize=18)\n",
                "# ax2.set_title(\"Fit\", )\n",
                "ax2.legend(title=\"Fit\", fontsize=16, title_fontsize=18)\n",
                "\n",
                "ax2.set_xticklabels([\"$10^0$\", \"$10^1$\", \"$10^2$\", \"$10^3$\", \"$10^4$\", \"$10^5$\"])\n",
                "# ax2.set_xlim(10, 100)\n",
                "\n",
                "fig.tight_layout(pad=0.25)\n",
                "\n",
                "# Already in log scale\n",
                "# train_ys, test_ys"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))\n",
                "\n",
                "ax1.plot(run[\"_step\"], run[\"test/acc\"], label=\"Test\", color=RED, linewidth=2)\n",
                "ax1.plot(run[\"_step\"], run[\"train/acc\"], label=\"Train\", color=BLUE, linewidth=2)\n",
                "ax1.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "# ax1.set_xlabel(\"Steps\", fontsize=18)\n",
                "ax1.set_xticklabels([\"\", \"\", \"\", \"\", \"\", \"\"], color=\"white\")\n",
                "ax1.set_xscale(\"log\")\n",
                "ax1.legend(title=\"Truth\", fontsize=16, title_fontsize=18)\n",
                "\n",
                "min_step, max_step = og_df[\"_step\"].min(), 100 # run[\"_step\"].max()\n",
                "\n",
                "ts = np.linspace(12, max_step, 1000)\n",
                "train_ys = [model(t).detach().numpy() for t in ts]\n",
                "test_ys = [model.test(t).detach().numpy() for t in ts]\n",
                "ax2.plot(ts, train_ys, label=\"Train\", color=BLUE, linewidth=2)\n",
                "ax2.plot(ts, test_ys, label=\"Test\", color=RED, linewidth=2)\n",
                "ax2.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "ax2.set_xlabel(\"Steps\", fontsize=18)\n",
                "# ax2.set_title(\"Fit\", )\n",
                "ax2.legend(title=\"Fit\", fontsize=16, title_fontsize=18)\n",
                "ax2.set_xticklabels([\"$10^0$\", \"$10^1$\", \"$10^2$\", \"$10^3$\", \"$10^4$\", \"$10^5$\"])\n",
                "# ax2.set_xlim(10, 100)\n",
                "\n",
                "fig.tight_layout(pad=0.25)\n",
                "\n",
                "# Already in log scale\n",
                "# train_ys, test_ys"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ignore any d_model < 50\n",
                "df_cleaned = df.loc[df[\"d_model\"] >= 50, :]\n",
                "d_models = df_cleaned.loc[:, \"d_model\"].unique()\n",
                "\n",
                "# Scaling analysis\n",
                "fig = plt.figure(figsize=(15, 4))\n",
                "ax = fig.add_subplot(111)\n",
                "\n",
                "colors = [BLUE, RED, \"green\"]\n",
                "y_max = 0\n",
                "\n",
                "for i in range(3):\n",
                "    slice = df_cleaned.loc[:, f\"pattern_{i}/onset\"]\n",
                "    y_max = max(y_max, slice.max())\n",
                "    ax.plot(d_models, slice, label=f\"\", color=colors[i], linewidth=2)\n",
                "\n",
                "ax.set_xlabel(\"d_model\", fontsize=18)\n",
                "ax.set_ylabel(\"Onset\", fontsize=18)\n",
                "\n",
                "\n",
                "# Fit a power-law to the onsets \n",
                "from scipy.optimize import curve_fit\n",
                "\n",
                "def power_law(x, a, b):\n",
                "    return a * x**b\n",
                "\n",
                "def fit_power_law(x, y):\n",
                "    popt, pcov = curve_fit(power_law, x, y)\n",
                "    return popt\n",
                "\n",
                "\n",
                "CUTOFF = 175\n",
                "\n",
                "# Fit power law to onset\n",
                "for i in range(3):\n",
                "    # Train up to a specific point\n",
                "    df_to_fit = df_cleaned.loc[df_cleaned[\"d_model\"] <= CUTOFF, :]\n",
                "    d_models_to_fit = df_to_fit.loc[:, \"d_model\"].unique()\n",
                "\n",
                "    onset_popt = fit_power_law(d_models_to_fit, df_to_fit.loc[:, f\"pattern_{i}/onset\"])\n",
                "    exponent = round(onset_popt[1], 2)\n",
                "    ax.plot(d_models, power_law(d_models, *onset_popt), label=f\"$\\\\nu_{i} = {exponent}$\", color=colors[i], linestyle=\"--\", linewidth=2)\n",
                "\n",
                "ax.vlines(CUTOFF, 0, y_max * 1.05, color=\"grey\", linestyle=\"--\", linewidth=2)\n",
                "ax.set_xlabel(\"Embedding dim.\", fontsize=18)\n",
                "ax.set_ylim(0, y_max * 1.05)\n",
                "\n",
                "ax.legend()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit the model & show the pattern. \n",
                "\n",
                "run_to_decompose = interp_sweep.loc[interp_sweep.lr_factor == 6, :]\n",
                "# Sort\n",
                "run_to_decompose.sort_values(by=\"_step\", inplace=True)\n",
                "steps = run_to_decompose._step\n",
                "\n",
                "# Apply a gaussian filter with sigma=2\n",
                "from scipy.ndimage import gaussian_filter1d\n",
                "\n",
                "run_to_decompose[\"test/acc\"] = gaussian_filter1d(run_to_decompose[\"test/acc\"], sigma=15)\n",
                "run_to_decompose[\"train/acc\"] = gaussian_filter1d(run_to_decompose[\"train/acc\"], sigma=15)\n",
                "\n",
                "plt.plot(steps, run_to_decompose[\"test/acc\"], label=\"Test\", color=RED, linewidth=2)\n",
                "plt.plot(steps, run_to_decompose[\"train/acc\"], label=\"Train\", color=BLUE, linewidth=2)\n",
                "\n",
                "plt.xscale(\"log\")\n",
                "\n",
                "run_to_decompose_rescaled = rescale_run(run_to_decompose, new_max=100)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = PatternLearningModel()\n",
                "\n",
                "def callback(x): \n",
                "    plot_patterns(x, run_to_decompose_rescaled)\n",
                "    plt.show()\n",
                "\n",
                "model.fit(run_to_decompose_rescaled, num_epochs=100, callback=callback, callback_ivl=25)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.ndimage import gaussian_filter1d\n",
                "\n",
                "r_uncorrupted = gaussian_filter1d(run[\"uncorrupted/acc\"], 2.)\n",
                "r_corrupted = gaussian_filter1d(run[\"corrupted/acc\"], 2.)\n",
                "\n",
                "r_uncorrupted"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(steps, run[\"corrupted/acc\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
                "\n",
                "type_1 = [(model.patterns[0](t) / model.patterns[0].strength).detach().float() for t in ts]\n",
                "type_2 = [(model.patterns[1](t) / model.patterns[1].strength).detach().float() for t in ts]\n",
                "type_3 = [(model.patterns[2](t) / model.patterns[2].strength).detach().float() for t in ts]\n",
                "\n",
                "TS = ts * 500_000 / 100\n",
                "\n",
                "# Plot 1: Uncorrupted data / Type 1 Pattern\n",
                "ax1.plot(steps, r_uncorrupted, label=\"Uncorrupted\", color=RED, linewidth=2)\n",
                "ax1.plot(TS, type_1, label=\"Prediction (TODO)\", color=BLUE, linestyle=\"-\", linewidth=2, alpha=0.75)\n",
                "ax1.legend(title=\"Type 1\", fontsize=12, title_fontsize=16, loc=\"upper left\")\n",
                "\n",
                "# TODO: Plot pattern 1\n",
                "# ax1.plot(steps, )\n",
                "\n",
                "# Plot 2: Corrupted data / Type 2 Pattern\n",
                "ax2.plot(steps, r_corrupted, label=\"Corrupted\", color=RED, linewidth=2)\n",
                "ax2.plot(TS, type_2, label=\"Prediction (TODO)\", color=BLUE, linestyle=\"-\", linewidth=2, alpha=0.75)\n",
                "ax2.legend(title=\"Type 2\", fontsize=12, title_fontsize=16, loc=\"upper left\")\n",
                "\n",
                "\n",
                "# Plot 3: Type 3 Pattern\n",
                "\n",
                "# ax3.plot(TS, type_3, label=\"Prediction (TODO)\", color=BLUE, linestyle=\"--\", linewidth=2)\n",
                "# ax3.legend(title=\"Type 3\", fontsize=12, title_fontsize=16, loc=\"upper left\")\n",
                "\n",
                "ax1.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "\n",
                "# for ax in [ax1, ax2, ax3]:\n",
                "for ax in [ax1, ax2]:\n",
                "    ax.set_xlabel(\"Steps\", fontsize=18)\n",
                "    ax.set_xscale(\"log\")\n",
                "    ax.set_xticklabels([\"\", \"\", \"$10^0$\", \"$10^1$\", \"$10^2$\", \"$10^3$\", \"$10^4$\", \"$10^5$\", ])\n",
                "\n",
                "plt.savefig(\"../figures/pattern-predictions.pdf\", bbox_inches=\"tight\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def get_history(\n",
                "    *sweep_ids,\n",
                "    unique_cols: Union[List[str], str] = \"weight_decay\",\n",
                "    entity: str = \"\",\n",
                "    project: str = \"grokking\",\n",
                "    allow_duplicates=False,\n",
                "    combine_seeds=False,\n",
                "):\n",
                "    \"\"\"\n",
                "    Gathers all the runs from a series of sweeps and combines them into a single dataframe.\n",
                "\n",
                "    `unique_col` is used to identify duplicate runs. By default, `\"_step\"` is added.\n",
                "    If there are duplicates, the run from the last sweep is kept.\n",
                "    \"\"\"\n",
                "    api = wandb.Api()\n",
                "    unique_cols = unique_cols if isinstance(unique_cols, list) else [unique_cols]\n",
                "\n",
                "    def _get_history(sweep_id):\n",
                "        \"\"\"Get a dataframe for a single sweep.\"\"\"\n",
                "        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
                "        runs = sweep.runs\n",
                "\n",
                "        def create_run_df(history, config):\n",
                "            for k, v in config.items():\n",
                "                if k == \"momentum\" and isinstance(v, list):\n",
                "                    v = [tuple(v)] * len(history)\n",
                "                history[k] = v\n",
                "\n",
                "            return history\n",
                "\n",
                "        return pd.concat([create_run_df(run.history(), run.config) for run in runs])\n",
                "\n",
                "    histories = pd.concat([_get_history(sweep_id) for sweep_id in sweep_ids])\n",
                "\n",
                "    if not allow_duplicates:\n",
                "        histories = histories.drop_duplicates([\"_step\", *unique_cols], keep=\"last\")\n",
                "\n",
                "    # Change step 0 to 1 to avoid issues with log plots\n",
                "    histories.loc[histories._step == 0, \"_step\"] = 1\n",
                "\n",
                "    # Fix types\n",
                "    histories.applymap(lambda x: x.item() if isinstance(x, np.generic) else x)\n",
                "    non_numeric_columns = histories.select_dtypes(\n",
                "        exclude=[\"int\", \"float\", \"int64\", \"float64\"]\n",
                "    ).columns\n",
                "    histories = histories.drop(columns=non_numeric_columns)\n",
                "\n",
                "    # Sort\n",
                "    histories = histories.sort_values(by=[*unique_cols, \"_step\"])\n",
                "\n",
                "    # Remove any runs that didn't have any steps after 1000\n",
                "    for unique_col in unique_cols:\n",
                "        valid_runs = histories.groupby(unique_col).apply(\n",
                "            lambda x: x[\"_step\"].max() > 1000\n",
                "        )\n",
                "        histories = histories[histories[unique_col].isin(valid_runs[valid_runs].index)]\n",
                "\n",
                "\n",
                "    return histories\n",
                "\n",
                "\n",
                "INTERP_SWEEPS = [\"kodd01ka\", \"wecya83q\", \"wqnakkjd\"]  # \"awxzpem1\"\n",
                "interp_sweep = get_history(*INTERP_SWEEPS, project=\"mnist-grokking\", allow_duplicates=True, combine_seeds=True)\n",
                "# interp_sweep.drop([\"weight/cos_sim_with_init\", \"test/efficiency\", \"train/efficiency\", \"weight/dist_from_init\"])\n",
                "interp_sweep"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "histories = interp_sweep.copy()\n",
                "unique_cols = [\"lr_factor\"]\n",
                "\n",
                "assert (\n",
                "    len(unique_cols) == 1\n",
                "), \"Can only combine seeds if there is a single unique column\"\n",
                "\n",
                "unique_col = unique_cols[0]\n",
                "unique_vals = histories[unique_col].unique()\n",
                "\n",
                "for val in unique_vals:\n",
                "    runs = histories[histories[unique_col] == val]\n",
                "    seeds = runs.seed.unique()\n",
                "\n",
                "    if len(seeds) > 1:\n",
                "        # Define the metrics that need to be averaged\n",
                "        metrics = [\"train/acc\", \"test/acc\", \"train/loss\", \"test/loss\", \"corrupted/acc\", \"uncorrupted/acc\"]\n",
                "        for metric in metrics:\n",
                "            # Calculate the mean value for each metric and _step\n",
                "            means_groups = runs.groupby(\"_step\")[metric]\n",
                "\n",
                "            means = means_groups.apply(\n",
                "                lambda x: x.ffill().bfill().mean() if x.isna().any() else x.mean()\n",
                "            )\n",
                "\n",
                "            if metric == \"corrupted/acc\":\n",
                "                print(means)\n",
                "\n",
                "            # Update the histories dataframe\n",
                "            for _step, mean_value in means.items():\n",
                "                mask = (histories[unique_col] == val) & (\n",
                "                    histories._step == _step\n",
                "                )\n",
                "                histories.loc[mask, metric] = mean_value\n",
                "\n",
                "# Remove duplicate rows\n",
                "histories = histories.drop_duplicates(subset=[*unique_cols, \"_step\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}