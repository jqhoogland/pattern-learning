{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install numpy torch sympy mod blobfile pandas seaborn matplotlib tqdm einops wandb\n",
                "\n",
                "import sys\n",
                "import os\n",
                "\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
                "\n",
                "from contextlib import suppress\n",
                "from dataclasses import dataclass, asdict\n",
                "from datetime import datetime\n",
                "from typing import Callable, Literal, Optional, Union, Tuple, List\n",
                "from copy import deepcopy\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "from torch import nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader\n",
                "from torch import optim\n",
                "import wandb\n",
                "\n",
                "if os.getenv('USE_TQDM_NOTEBOOK', 'NO').lower() in ['yes', 'true', '1']:\n",
                "    from tqdm.notebook import tqdm\n",
                "else:\n",
                "    from tqdm import tqdm\n",
                "\n",
                "import ipywidgets as widgets\n",
                "import wandb\n",
                "\n",
                "import matplotlib as mpl\n",
                "from matplotlib.colors import LogNorm\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from patterns.dataset import ModularArithmetic, Operator\n",
                "from patterns.transformer import Transformer\n",
                "from patterns.utils import generate_run_name\n",
                "from patterns.learner import Config\n",
                "\n",
                "from toy_models.fit import rescale_run, Pattern, PatternLearningModel\n",
                "from unifying.sweep import get_history, handle_outliers\n",
                "from unifying.plotting import BLUE, RED\n",
                "\n",
                "DEFAULT_MODULUS = 113\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "sns.set_theme(style=\"darkgrid\")\n",
                "\n",
                "ENTITY = \"INSERT ENTITY HERE\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_patterns(pl_model, run, log=False):\n",
                "    ts = run[\"_step\"].values\n",
                "    train_preds = [pl_model(t).detach().numpy() for t in ts]\n",
                "    test_preds = [pl_model.test(t).detach().numpy() for t in ts]\n",
                "    train_ys = torch.tensor(run[\"train/acc\"].values).float()\n",
                "    test_ys = torch.tensor(run[\"test/acc\"].values).float()\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
                "\n",
                "    axes[0].plot(ts, train_preds, label=\"train\", color=\"blue\")\n",
                "    axes[0].plot(ts, test_preds, label=\"test\", color=\"red\")\n",
                "\n",
                "    axes[1].plot(ts, train_ys, label=\"train\", color=\"blue\")\n",
                "    axes[1].plot(ts, test_ys, label=\"test\", color=\"red\")\n",
                "\n",
                "    axes[0].set_title(\"Predictions\")\n",
                "    axes[1].set_title(\"True values\")\n",
                "\n",
                "    if log:\n",
                "        axes[0].set_xscale(\"log\")\n",
                "        axes[1].set_xscale(\"log\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unifying.sweep import METRICS, get_pivot\n",
                "from scipy.ndimage import gaussian_filter\n",
                "\n",
                "def get_mw_sweep(metrics=[\"test/acc\", \"train/acc\", \"test/loss\", \"train/loss\"], steps=None, num_steps=2000):\n",
                "    df = pd.read_csv(\"../logs/mw_sweep.csv\")\n",
                "\n",
                "    MINIMUM = 1/97\n",
                "    missing_row = df.loc[df._step == 60, :].copy()\n",
                "    missing_row._step = 1\n",
                "    missing_row[\"train/acc\"] = MINIMUM\n",
                "    missing_row[\"test/acc\"] = MINIMUM\n",
                "    df = pd.concat(\n",
                "        [\n",
                "            missing_row,\n",
                "            df,\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    d_models = sorted(df[\"d_model\"].unique())\n",
                "    print(d_models)\n",
                "    df = get_pivot(df, \"d_model\", metrics, reindex=True, interpolate=True) \n",
                "\n",
                "    steps = steps or list({int(s) for s in np.logspace(0, np.log10(df.index.max()), num_steps)})\n",
                "    df = df.loc[df.index.isin(steps), :]\n",
                "\n",
                "    df = df.reset_index()  # to make _step a regular column\n",
                "    df.columns = ['_step'] + [f'{x}_{y}' for x, y in df.columns[1:]]  # to make columns single level\n",
                "    df_melted = df.melt(id_vars='_step', var_name='variable', value_name='value')\n",
                "\n",
                "    # split the variable column into the original columns\n",
                "    df_melted[['metric', 'd_model']] = df_melted['variable'].str.split('_', expand=True)\n",
                "    df_melted['d_model'] = df_melted['d_model'].astype(int)  # convert d_model back to integer\n",
                "\n",
                "    # pivot to get the original columns\n",
                "    df_final = df_melted.pivot(index=['_step', 'd_model'], columns='metric', values='value').reset_index()\n",
                "\n",
                "    return df_final\n",
                "\n",
                "df = get_mw_sweep(num_steps=500)\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "D_MODEL = 128\n",
                "run = df.loc[df.d_model == D_MODEL, :]\n",
                "run"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(run._step, run[\"train/acc\"], label=\"train\")\n",
                "# plt.plot(run.index, run[\"train/acc\"], label=\"smoothed\")\n",
                "plt.plot(run._step, run[\"test/acc\"], label=\"test\")\n",
                "# plt.plot(run.index, run[\"test/acc\"], label=\"smoothed\")\n",
                "plt.xscale(\"log\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rescaled_run = rescale_run(run, new_max=100., log=False) \n",
                "\n",
                "# for metric in METRICS:\n",
                "#     rescaled_run.loc[metric, :] = gaussian_filter1d(rescaled_run.loc[metric,:])\n",
                "\n",
                "rescaled_run.plot(x=\"_step\", y=[\"train/acc\", \"test/acc\"], logx=True, figsize=(10, 5))\n",
                "rescaled_run"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = PatternLearningModel(max_time=100., num_patterns=3)\n",
                "\n",
                "# Initialization\n",
                "for i, pattern in enumerate(model.patterns):\n",
                "    max_time = 100\n",
                "    pattern.onset.data = torch.tensor(0.1 * (0.25 * max_time) ** i)\n",
                "    pattern.speed.data = torch.tensor((max_time / 2) * 10 ** (-i))\n",
                "    # pattern._strength.data = pattern._inv_sigmoid(torch.tensor([.8, 1.0, 1.0][i]))\n",
                "    # pattern._generalization.data = torch.log(torch.tensor([.3, 0.01, .69][i]))\n",
                "\n",
                "print(model.patterns)\n",
                "\n",
                "def callback(x): \n",
                "    plot_patterns(x, rescaled_run, log=True)\n",
                "    plt.show()\n",
                "\n",
                "callback(model)\n",
                "\n",
                "model.fit(rescaled_run, lr=0.1, num_epochs=500, callback=callback, callback_ivl=25)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "VARIABLE_COLS = [\n",
                "    \"test/acc\",\n",
                "    \"train/acc\",\n",
                "    \"test/loss\",\n",
                "    \"train/loss\",\n",
                "    \"_step\",\n",
                "    \"weight/norm\",\n",
                "    \"test/efficiency\",\n",
                "    \"train/efficiency\",\n",
                "    \"weight/dist_from_init\",\n",
                "    \"weight/cos_sim_with_init\",\n",
                "]\n",
                "\n",
                "def fit_sweep(df: pd.DataFrame, unique_col: str, lr=0.1, max_time=1.0, num_patterns=3, num_epochs=500, log=False, **kwargs):\n",
                "    unique_vals = df.loc[:, unique_col].unique()\n",
                "\n",
                "    variable_cols = [c for c in df.columns if c in VARIABLE_COLS]\n",
                "    hyperparams: dict = (\n",
                "        df.loc[0, :]\n",
                "        .drop(columns=[unique_col, *variable_cols])\n",
                "        .to_dict()\n",
                "    )\n",
                "\n",
                "    wandb.init(\n",
                "        project=\"fit-toy-model\",\n",
                "    )\n",
                "\n",
                "    try:\n",
                "        for unique_val in tqdm(unique_vals):\n",
                "            run = df.loc[df[unique_col] == unique_val]\n",
                "            rescaled_run = rescale_run(run, new_max=max_time, log=log)\n",
                "\n",
                "            pl_model = PatternLearningModel(\n",
                "                num_patterns=num_patterns, \n",
                "                max_time=max_time\n",
                "            )\n",
                "\n",
                "            def _plot_patterns(pl_model):\n",
                "                plot_patterns(pl_model, rescaled_run)\n",
                "                plt.show()\n",
                "\n",
                "            pl_model.fit(rescaled_run, lr=lr, num_epochs=num_epochs, callback=_plot_patterns)\n",
                "            pl_model.rescale(1.)\n",
                "\n",
                "            wandb.log({unique_col: unique_val, **pl_model.to_dict(), **hyperparams, **kwargs})\n",
                "\n",
                "            _plot_patterns(pl_model)\n",
                "            plt.show()\n",
                "\n",
                "    except KeyboardInterrupt:\n",
                "        wandb.finish()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find the first _step for each run (= unique d_model) where the test/acc > 0.5 in df\n",
                "onsets = df.loc[df[\"test/acc\"] > 0.5, :].groupby(\"d_model\").first()\n",
                "d_models = onsets.index.values\n",
                "onsets = onsets[\"_step\"].values\n",
                "onsets = gaussian_filter(onsets, 2)\n",
                "onsets_df = pd.DataFrame([{\"d_model\": d_model, \"step\": step} for d_model, step in zip(d_models, onsets)])\n",
                "\n",
                "# Fit a straight line in log-y space to the onsets_df (all samples before d_model =100)\n",
                "onsets_train = onsets_df.loc[onsets_df.d_model < 120, :]\n",
                "onsets_train[\"log_step\"] = np.log(onsets_train[\"step\"])\n",
                "\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "from sklearn.pipeline import make_pipeline\n",
                "\n",
                "model = make_pipeline(PolynomialFeatures(1), LinearRegression())\n",
                "model.fit(onsets_train[[\"d_model\"]], onsets_train[\"log_step\"])\n",
                "\n",
                "# Plot the onsets_df and the fitted line\n",
                "plt.plot(onsets_df[\"d_model\"], onsets_df[\"step\"], label=\"Truth\")\n",
                "plt.plot(onsets_df[\"d_model\"], np.exp(model.predict(onsets_df[[\"d_model\"]])), label=\"Fit\")\n",
                "plt.yscale(\"log\")\n",
                "plt.vlines(120, 1, 500_000, label=\"__hidden\", linestyle=\"--\")\n",
                "plt.ylim(1, 500_000)\n",
                "plt.ylabel(\"Onset step\", fontsize=16)\n",
                "plt.xlabel(\"Emedding dim.\", fontsize=16)\n",
                "plt.legend()\n",
                "# onsets_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "unique_col = \"d_model\"\n",
                "unique_vals = df.loc[:, unique_col].unique() \n",
                "unique_vals"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fit_sweep(df, \"d_model\", max_time=100., num_patterns=2, num_epochs=500, log=False)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fit the sweeps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "api = wandb.Api()\n",
                "runs = api.runs(f\"{ENTITY}/fit-toy-model\")\n",
                "[run for run in runs]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run = runs[0]\n",
                "df = run.history()\n",
                "\n",
                "for step in df._step.unique():\n",
                "    df.loc[df._step == step, \"d_model\"] = unique_vals[step]\n",
                "\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.plot(x=\"d_model\", y=[\"pattern_0/onset\", \"pattern_1/onset\"], figsize=(10, 5))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "col = \"d_model\"\n",
                "unique_vals = df.loc[:, col].unique()\n",
                "print(unique_vals)\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def df_row_to_toy_model(row):\n",
                "    model = PatternLearningModel(max_time=1.)\n",
                "\n",
                "    for i, pattern in enumerate(model.patterns):\n",
                "        pattern.onset.data = torch.tensor(row[f\"pattern_{i}/onset\"])\n",
                "        pattern.speed.data = torch.tensor(row[f\"pattern_{i}/speed\"])\n",
                "        pattern._strength.data = pattern._inv_sigmoid(torch.tensor(row[f\"pattern_{i}/strength\"]))  # type: ignore\n",
                "        pattern._generalization.data = torch.log(torch.tensor(row[f\"pattern_{i}/generalization\"]))\n",
                "\n",
                "    return model\n",
                "\n",
                "D_MODEL = 115\n",
                "co9l = \"d_model\"\n",
                "model_entry = df.loc[df[col] == D_MODEL, :].iloc[0, :]\n",
                "print(model_entry)\n",
                "model = df_row_to_toy_model(model_entry)\n",
                "model.rescale(100)\n",
                "model.patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get corresponding original run\n",
                "og_df = get_history(DM_SWEEP_ID, unique_cols=\"d_model\")\n",
                "run = og_df.loc[og_df.d_model==D_MODEL,:] #.plot(x=\"_step\", y=\"test/acc\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))\n",
                "\n",
                "ax1.plot(run[\"_step\"], run[\"test/acc\"], label=\"Test\", color=RED, linewidth=2)\n",
                "ax1.plot(run[\"_step\"], run[\"train/acc\"], label=\"Train\", color=BLUE, linewidth=2)\n",
                "ax1.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "# ax1.set_xlabel(\"Steps\", fontsize=18)\n",
                "ax1.set_xticklabels([\"\", \"\", \"\", \"\", \"\", \"\"], color=\"white\")\n",
                "ax1.set_xscale(\"log\")\n",
                "ax1.legend(title=\"Truth\", fontsize=16, title_fontsize=18)\n",
                "\n",
                "min_step, max_step = og_df[\"_step\"].min(), 100 # run[\"_step\"].max()\n",
                "\n",
                "ts = np.linspace(12, max_step, 1000)\n",
                "train_ys = [model(t).detach().numpy() for t in ts]\n",
                "test_ys = [model.test(t).detach().numpy() for t in ts]\n",
                "ax2.plot(ts, train_ys, label=\"Train\", color=BLUE, linewidth=2)\n",
                "ax2.plot(ts, test_ys, label=\"Test\", color=RED, linewidth=2)\n",
                "ax2.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "ax2.set_xlabel(\"Steps\", fontsize=18)\n",
                "# ax2.set_title(\"Fit\", )\n",
                "ax2.legend(title=\"Fit\", fontsize=16, title_fontsize=18)\n",
                "\n",
                "ax2.set_xticklabels([\"$10^0$\", \"$10^1$\", \"$10^2$\", \"$10^3$\", \"$10^4$\", \"$10^5$\"])\n",
                "# ax2.set_xlim(10, 100)\n",
                "\n",
                "fig.tight_layout(pad=0.25)\n",
                "\n",
                "# Already in log scale\n",
                "# train_ys, test_ys"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8))\n",
                "\n",
                "ax1.plot(run[\"_step\"], run[\"test/acc\"], label=\"Test\", color=RED, linewidth=2)\n",
                "ax1.plot(run[\"_step\"], run[\"train/acc\"], label=\"Train\", color=BLUE, linewidth=2)\n",
                "ax1.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "# ax1.set_xlabel(\"Steps\", fontsize=18)\n",
                "ax1.set_xticklabels([\"\", \"\", \"\", \"\", \"\", \"\"], color=\"white\")\n",
                "ax1.set_xscale(\"log\")\n",
                "ax1.legend(title=\"Truth\", fontsize=16, title_fontsize=18)\n",
                "\n",
                "min_step, max_step = og_df[\"_step\"].min(), 100 # run[\"_step\"].max()\n",
                "\n",
                "ts = np.linspace(12, max_step, 1000)\n",
                "train_ys = [model(t).detach().numpy() for t in ts]\n",
                "test_ys = [model.test(t).detach().numpy() for t in ts]\n",
                "ax2.plot(ts, train_ys, label=\"Train\", color=BLUE, linewidth=2)\n",
                "ax2.plot(ts, test_ys, label=\"Test\", color=RED, linewidth=2)\n",
                "ax2.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "ax2.set_xlabel(\"Steps\", fontsize=18)\n",
                "# ax2.set_title(\"Fit\", )\n",
                "ax2.legend(title=\"Fit\", fontsize=16, title_fontsize=18)\n",
                "ax2.set_xticklabels([\"$10^0$\", \"$10^1$\", \"$10^2$\", \"$10^3$\", \"$10^4$\", \"$10^5$\"])\n",
                "# ax2.set_xlim(10, 100)\n",
                "\n",
                "fig.tight_layout(pad=0.25)\n",
                "\n",
                "# Already in log scale\n",
                "# train_ys, test_ys"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ignore any d_model < 50\n",
                "df_cleaned = df.loc[df[\"d_model\"] >= 50, :]\n",
                "d_models = df_cleaned.loc[:, \"d_model\"].unique()\n",
                "\n",
                "# Scaling analysis\n",
                "fig = plt.figure(figsize=(15, 4))\n",
                "ax = fig.add_subplot(111)\n",
                "\n",
                "colors = [BLUE, RED, \"green\"]\n",
                "y_max = 0\n",
                "\n",
                "for i in range(3):\n",
                "    slice = df_cleaned.loc[:, f\"pattern_{i}/onset\"]\n",
                "    y_max = max(y_max, slice.max())\n",
                "    ax.plot(d_models, slice, label=f\"\", color=colors[i], linewidth=2)\n",
                "\n",
                "ax.set_xlabel(\"d_model\", fontsize=18)\n",
                "ax.set_ylabel(\"Onset\", fontsize=18)\n",
                "\n",
                "\n",
                "# Fit a power-law to the onsets \n",
                "from scipy.optimize import curve_fit\n",
                "\n",
                "def power_law(x, a, b):\n",
                "    return a * x**b\n",
                "\n",
                "def fit_power_law(x, y):\n",
                "    popt, pcov = curve_fit(power_law, x, y)\n",
                "    return popt\n",
                "\n",
                "\n",
                "CUTOFF = 175\n",
                "\n",
                "# Fit power law to onset\n",
                "for i in range(3):\n",
                "    # Train up to a specific point\n",
                "    df_to_fit = df_cleaned.loc[df_cleaned[\"d_model\"] <= CUTOFF, :]\n",
                "    d_models_to_fit = df_to_fit.loc[:, \"d_model\"].unique()\n",
                "\n",
                "    onset_popt = fit_power_law(d_models_to_fit, df_to_fit.loc[:, f\"pattern_{i}/onset\"])\n",
                "    exponent = round(onset_popt[1], 2)\n",
                "    ax.plot(d_models, power_law(d_models, *onset_popt), label=f\"$\\\\nu_{i} = {exponent}$\", color=colors[i], linestyle=\"--\", linewidth=2)\n",
                "\n",
                "ax.vlines(CUTOFF, 0, y_max * 1.05, color=\"grey\", linestyle=\"--\", linewidth=2)\n",
                "ax.set_xlabel(\"Embedding dim.\", fontsize=18)\n",
                "ax.set_ylim(0, y_max * 1.05)\n",
                "\n",
                "ax.legend()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy.ndimage import gaussian_filter1d\n",
                "\n",
                "r_uncorrupted = gaussian_filter1d(run[\"uncorrupted/acc\"], 2.)\n",
                "r_corrupted = gaussian_filter1d(run[\"corrupted/acc\"], 2.)\n",
                "\n",
                "r_uncorrupted"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(steps, run[\"corrupted/acc\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
                "\n",
                "type_1 = [(model.patterns[0](t) / model.patterns[0].strength).detach().float() for t in ts]\n",
                "type_2 = [(model.patterns[1](t) / model.patterns[1].strength).detach().float() for t in ts]\n",
                "type_3 = [(model.patterns[2](t) / model.patterns[2].strength).detach().float() for t in ts]\n",
                "\n",
                "TS = ts * 500_000 / 100\n",
                "\n",
                "# Plot 1: Uncorrupted data / Type 1 Pattern\n",
                "ax1.plot(steps, r_uncorrupted, label=\"Uncorrupted\", color=RED, linewidth=2)\n",
                "ax1.plot(TS, type_1, label=\"Prediction\", color=BLUE, linestyle=\"-\", linewidth=2, alpha=0.75)\n",
                "ax1.legend(title=\"Type 1\", fontsize=12, title_fontsize=16, loc=\"upper left\")\n",
                "\n",
                "# TODO: Plot pattern 1\n",
                "# ax1.plot(steps, )\n",
                "\n",
                "# Plot 2: Corrupted data / Type 2 Pattern\n",
                "ax2.plot(steps, r_corrupted, label=\"Corrupted\", color=RED, linewidth=2)\n",
                "ax2.plot(TS, type_2, label=\"Prediction\", color=BLUE, linestyle=\"-\", linewidth=2, alpha=0.75)\n",
                "ax2.legend(title=\"Type 2\", fontsize=12, title_fontsize=16, loc=\"upper left\")\n",
                "\n",
                "\n",
                "# Plot 3: Type 3 Pattern\n",
                "\n",
                "# ax3.plot(TS, type_3, label=\"Prediction (TODO)\", color=BLUE, linestyle=\"--\", linewidth=2)\n",
                "# ax3.legend(title=\"Type 3\", fontsize=12, title_fontsize=16, loc=\"upper left\")\n",
                "\n",
                "ax1.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "\n",
                "# for ax in [ax1, ax2, ax3]:\n",
                "for ax in [ax1, ax2]:\n",
                "    ax.set_xlabel(\"Steps\", fontsize=18)\n",
                "    ax.set_xscale(\"log\")\n",
                "    ax.set_xticklabels([\"\", \"\", \"$10^0$\", \"$10^1$\", \"$10^2$\", \"$10^3$\", \"$10^4$\", \"$10^5$\", ])\n",
                "\n",
                "plt.savefig(\"../figures/pattern-predictions.pdf\", bbox_inches=\"tight\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "INTERP_SWEEPS = [\"kodd01ka\", \"wecya83q\", \"wqnakkjd\"]  # \"awxzpem1\"\n",
                "interp_sweep = get_history(*INTERP_SWEEPS, project=\"mnist-grokking\", allow_duplicates=True, combine_seeds=True)\n",
                "# interp_sweep.drop([\"weight/cos_sim_with_init\", \"test/efficiency\", \"train/efficiency\", \"weight/dist_from_init\"])\n",
                "interp_sweep"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "histories = interp_sweep.copy()\n",
                "unique_cols = [\"lr_factor\"]\n",
                "\n",
                "assert (\n",
                "    len(unique_cols) == 1\n",
                "), \"Can only combine seeds if there is a single unique column\"\n",
                "\n",
                "unique_col = unique_cols[0]\n",
                "unique_vals = histories[unique_col].unique()\n",
                "\n",
                "for val in unique_vals:\n",
                "    runs = histories[histories[unique_col] == val]\n",
                "    seeds = runs.seed.unique()\n",
                "\n",
                "    if len(seeds) > 1:\n",
                "        # Define the metrics that need to be averaged\n",
                "        metrics = [\"train/acc\", \"test/acc\", \"train/loss\", \"test/loss\", \"corrupted/acc\", \"uncorrupted/acc\"]\n",
                "        for metric in metrics:\n",
                "            # Calculate the mean value for each metric and _step\n",
                "            means_groups = runs.groupby(\"_step\")[metric]\n",
                "\n",
                "            means = means_groups.apply(\n",
                "                lambda x: x.ffill().bfill().mean() if x.isna().any() else x.mean()\n",
                "            )\n",
                "\n",
                "            if metric == \"corrupted/acc\":\n",
                "                print(means)\n",
                "\n",
                "            # Update the histories dataframe\n",
                "            for _step, mean_value in means.items():\n",
                "                mask = (histories[unique_col] == val) & (\n",
                "                    histories._step == _step\n",
                "                )\n",
                "                histories.loc[mask, metric] = mean_value\n",
                "\n",
                "# Remove duplicate rows\n",
                "histories = histories.drop_duplicates(subset=[*unique_cols, \"_step\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
