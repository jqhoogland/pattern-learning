{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install numpy torch sympy mod blobfile pandas seaborn matplotlib tqdm einops wandb\n",
                "\n",
                "import sys\n",
                "import os\n",
                "\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
                "\n",
                "from contextlib import suppress\n",
                "from dataclasses import dataclass, asdict\n",
                "from datetime import datetime\n",
                "from typing import Callable, Literal, Optional, Union, Tuple, List\n",
                "from copy import deepcopy\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "from torch import nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader\n",
                "from torch import optim\n",
                "import wandb\n",
                "from tqdm.notebook import tqdm\n",
                "import ipywidgets as widgets\n",
                "import wandb\n",
                "\n",
                "import matplotlib as mpl\n",
                "from matplotlib.colors import LogNorm\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from patterns.dataset import ModularArithmetic, Operator\n",
                "from patterns.transformer import Transformer\n",
                "from patterns.utils import generate_run_name\n",
                "from patterns.learner import Config\n",
                "\n",
                "from toy_models.fit import rescale_run, Pattern, PatternLearningModel\n",
                "\n",
                "from unifying.sweep import get_history, handle_outliers\n",
                "from unifying.plotting import BLUE, RED\n",
                "\n",
                "DEFAULT_MODULUS = 113\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "sns.set_theme(style=\"darkgrid\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from contextlib import suppress\n",
                "from copy import deepcopy\n",
                "from dataclasses import asdict, dataclass\n",
                "from datetime import datetime\n",
                "from typing import Callable, List, Literal, Optional, Tuple, Union\n",
                "\n",
                "import ipywidgets as widgets\n",
                "import matplotlib as mpl\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "from matplotlib.colors import LogNorm\n",
                "from torch import nn, optim\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "import wandb\n",
                "\n",
                "\n",
                "def rescale_run(run, new_max=1.0, log=True):\n",
                "    # Changes the steps to fit in the range [0, 100] (following a log scale)\n",
                "    run = run.copy()\n",
                "    max_ = run[\"_step\"].max()\n",
                "\n",
                "    if log:\n",
                "        max_ = np.log(max_)\n",
                "        run[\"_step\"] = np.log(run[\"_step\"]) / max_ * new_max\n",
                "    else:\n",
                "        run[\"_step\"] = run[\"_step\"] / max_ * new_max\n",
                "\n",
                "    return run\n",
                "\n",
                "\n",
                "class Pattern(nn.Module):\n",
                "    def __init__(self, max_time: float = 1.0, onset: Optional[float] = None, generalization: Optional[float] = None, strength: Optional[float] = None, speed: Optional[float] = None):\n",
                "        # 4 scalar parameters: strength, speed, onset, generalization\n",
                "        super().__init__()\n",
                "\n",
                "        strength = strength or torch.rand(1)[0]\n",
                "        speed = speed or torch.rand(1)[0] * 10 / max_time\n",
                "        onset = onset or torch.rand(1)[0] * max_time\n",
                "        generalization = generalization or torch.rand(1)[0]\n",
                "\n",
                "        self._strength = nn.Parameter(self._inv_sigmoid(torch.tensor(strength)))\n",
                "        self.speed = nn.Parameter(torch.tensor(speed))\n",
                "        self.onset = nn.Parameter(torch.tensor(onset))\n",
                "        self._generalization = nn.Parameter(torch.log(torch.tensor(generalization)))\n",
                "\n",
                "    @staticmethod\n",
                "    def _inv_sigmoid(x):\n",
                "        return torch.log(x / (1 - x))\n",
                "    \n",
                "    @property\n",
                "    def strength(self):\n",
                "        return F.sigmoid(self._strength)\n",
                "    \n",
                "    @strength.setter\n",
                "    def strength(self, value):\n",
                "        self._strength = self._inv_sigmoid(value)\n",
                "\n",
                "    @property\n",
                "    def generalization(self):\n",
                "        return torch.exp(self._generalization)\n",
                "    \n",
                "    @generalization.setter\n",
                "    def generalization(self, value):\n",
                "        self._generalization = torch.log(value)\n",
                "\n",
                "    def forward(self, t):\n",
                "        return self.strength * F.sigmoid(self.speed * (t - self.onset))\n",
                "\n",
                "    def __repr__(self):\n",
                "        return f\"Pattern(strength={self.strength.data.float()}, speed={self.speed.data.float()}, onset={self.onset.data.float()}, generalization={self.generalization.data.float()})\"\n",
                "\n",
                "\n",
                "class PatternLearningModel(nn.Module):\n",
                "    def __init__(self, num_patterns: int = 3, max_time=1.0):\n",
                "        super().__init__()\n",
                "        self.num_patterns = num_patterns\n",
                "        self.patterns = nn.ModuleList([\n",
                "            Pattern(\n",
                "                max_time, \n",
                "                onset=max_time * (i + 1) / (num_patterns + 1),\n",
                "                speed=10./max_time,\n",
                "                generalization=0.5,\n",
                "                strength=0.5\n",
                "            ) \n",
                "            for i in range(num_patterns)\n",
                "        ])\n",
                "        self.max_time = max_time\n",
                "\n",
                "        self.binary_mask = torch.tensor(\n",
                "            [\n",
                "                [int(i) for i in bin(j)[2:].zfill(num_patterns)]\n",
                "                for j in range(2**num_patterns)\n",
                "            ]\n",
                "        ).float()\n",
                "\n",
                "        self.counts = self.binary_mask.sum(dim=1)\n",
                "\n",
                "    def forward(self, t):\n",
                "        return 1 - torch.prod(1 - self.predictivenesses(t), dim=0)\n",
                "\n",
                "    # def usages(self, t):\n",
                "    #     preds = self.predictivenesses(t)\n",
                "    #     usages = torch.prod(preds.T * self.binary_mask + (1 - preds.T) * (1 - self.binary_mask), dim=1)\n",
                "    #     return usages\n",
                "\n",
                "    def gs(self):\n",
                "        return torch.stack([p.generalization for p in self.patterns])\n",
                "\n",
                "    # def generalizations(self):\n",
                "    #     generalizations = torch.sum(self.gs().T * self.binary_mask, dim=1) / self.counts\n",
                "    #     generalizations[0] = 0\n",
                "    #     return generalizations\n",
                "\n",
                "    def predictivenesses(self, t):\n",
                "        return torch.stack([p(t) for p in self.patterns])\n",
                "\n",
                "    def forward(self, t):\n",
                "        prod = 1\n",
                "\n",
                "        for p in self.patterns:\n",
                "            prod *= 1 - p(t)\n",
                "\n",
                "        return 1 - prod\n",
                "\n",
                "    def usages(self, t):\n",
                "        preds = [p(t) for p in self.patterns]\n",
                "        usages = torch.ones(2**self.num_patterns)\n",
                "\n",
                "        for i in range(2**self.num_patterns):\n",
                "            for j in range(self.num_patterns):\n",
                "                if i & (1 << j):\n",
                "                    usages[i] *= preds[j]\n",
                "                else:\n",
                "                    usages[i] *= 1 - preds[j]\n",
                "\n",
                "        return usages\n",
                "\n",
                "    def generalizations(self):\n",
                "        generalizations = torch.zeros(2**self.num_patterns)\n",
                "\n",
                "        for i in range(2**self.num_patterns):\n",
                "            count = 0\n",
                "            total = 0\n",
                "\n",
                "            for j in range(self.num_patterns):\n",
                "                if i & (1 << j):\n",
                "                    # print(i, j, self.patterns[j].generalization, generalizations[i])\n",
                "                    generalizations[i] += self.patterns[j].generalization\n",
                "                    count += 1\n",
                "                \n",
                "                total += self.patterns[j].generalization\n",
                "\n",
                "            # if count > 0:\n",
                "            #     generalizations[i] /= count\n",
                "\n",
                "            if total > 0:\n",
                "                generalizations[i] /= total\n",
                "\n",
                "        return generalizations\n",
                "\n",
                "    def test(self, t):\n",
                "        return torch.sum(self.generalizations() * self.usages(t), dim=0)\n",
                "\n",
                "    def fit(self, run, lr=0.1, num_epochs=1000, callback=None, callback_ivl=100):\n",
                "        ts = torch.tensor(run._step.values).float()\n",
                "\n",
                "        train_ys = torch.tensor(run[\"train/acc\"].values).float()\n",
                "        test_ys = torch.tensor(run[\"test/acc\"].values).float()\n",
                "\n",
                "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
                "        criterion = nn.MSELoss()\n",
                "        # Cross-entropy\n",
                "        eps = 1e-6\n",
                "        # criterion = lambda preds, ys: -torch.sum(ys * torch.log(preds + eps) + (1 - ys) * torch.log(1 - preds + eps))\n",
                "        callback(self)\n",
                "\n",
                "        for epoch in tqdm(range(num_epochs)):\n",
                "            train_preds = torch.zeros_like(train_ys)\n",
                "            test_preds = torch.zeros_like(test_ys)\n",
                "\n",
                "            optimizer.zero_grad()\n",
                "\n",
                "            for i, t in enumerate(ts):\n",
                "                train_preds[i] = self(t)\n",
                "                test_preds[i] = self.test(t)\n",
                "\n",
                "            loss = criterion(train_preds, train_ys) + criterion(test_preds, test_ys)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "\n",
                "            print(f\"Epoch {epoch} - loss: {loss.item()}\")\n",
                "            \n",
                "            if callback is not None and epoch % callback_ivl == 0:\n",
                "                callback(self)\n",
                "\n",
                "        return self\n",
                "\n",
                "    def to_dict(self):\n",
                "        \"\"\"To a dataframe, sorting patterns by onset time\"\"\"\n",
                "        patterns = sorted(self.patterns, key=lambda p: p.onset.data)\n",
                "        d = {}\n",
                "\n",
                "        for i, p in enumerate(patterns):\n",
                "            d[f\"pattern_{i}/strength\"] = p.strength.data\n",
                "            d[f\"pattern_{i}/speed\"] = p.speed.data\n",
                "            d[f\"pattern_{i}/onset\"] = p.onset.data\n",
                "            d[f\"pattern_{i}/generalization\"] = p.generalization.data\n",
                "\n",
                "        return d\n",
                "    \n",
                "    def rescale(self, max_time):\n",
                "        \"\"\"Rescale the model to a new max time\"\"\"\n",
                "        scaling_factor = max_time / self.max_time\n",
                "\n",
                "        for p in self.patterns:\n",
                "            p.onset.data /= scaling_factor\n",
                "            p.speed.data *= scaling_factor\n",
                "\n",
                "        self.max_time = max_time\n",
                "\n",
                "    def __repr__(self):\n",
                "        return f\"PatternLearningModel({self.to_dict()})\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.manual_seed(2)\n",
                "pl_model = PatternLearningModel(max_time=100.)\n",
                "\n",
                "def plot_patterns(pl_model, run):\n",
                "    ts = run[\"_step\"].values\n",
                "    train_preds = [pl_model(t).detach().numpy() for t in ts]\n",
                "    test_preds = [pl_model.test(t).detach().numpy() for t in ts]\n",
                "    train_ys = torch.tensor(run[\"train/acc\"].values).float()\n",
                "    test_ys = torch.tensor(run[\"test/acc\"].values).float()\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
                "\n",
                "    axes[0].plot(ts, train_preds, label=\"train\", color=\"blue\")\n",
                "    axes[0].plot(ts, test_preds, label=\"test\", color=\"red\")\n",
                "\n",
                "    axes[1].plot(ts, train_ys, label=\"train\", color=\"blue\")\n",
                "    axes[1].plot(ts, test_ys, label=\"test\", color=\"red\")\n",
                "\n",
                "    axes[0].set_title(\"Predictions\")\n",
                "    axes[1].set_title(\"True values\")\n",
                "\n",
                "    # axes[0].set_xscale(\"log\")\n",
                "    # axes[1].set_xscale(\"log\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def _plot_patterns(pl_model):\n",
                "    plot_patterns(pl_model, rescaled_run)\n",
                "    plt.show()\n",
                "\n",
                "    print(pl_model.patterns)\n",
                "\n",
                "\n",
                "pl_model = PatternLearningModel(num_patterns=3, max_time=100.)\n",
                "# pl_model.fit(rescaled_run, lr=0.1, callback=_plot_patterns, callback_ivl=10, num_epochs=500)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "VARIABLE_COLS = [\n",
                "    \"test/acc\",\n",
                "    \"train/acc\",\n",
                "    \"test/loss\",\n",
                "    \"train/loss\",\n",
                "    \"_step\",\n",
                "    \"weight/norm\",\n",
                "    \"test/efficiency\",\n",
                "    \"train/efficiency\",\n",
                "    \"weight/dist_from_init\",\n",
                "    \"weight/cos_sim_with_init\",\n",
                "]\n",
                "\n",
                "def fit_sweep(df: pd.DataFrame, unique_col: str, lr=0.1, max_time=1.0, num_patterns=3, num_epochs=500, **kwargs):\n",
                "    unique_vals = df.loc[:, unique_col].unique()\n",
                "\n",
                "    variable_cols = [c for c in df.columns if c in VARIABLE_COLS]\n",
                "    hyperparams: dict = (\n",
                "        df.loc[0, :]\n",
                "        .drop(columns=[unique_col, *variable_cols])\n",
                "        .to_dict()\n",
                "    )\n",
                "\n",
                "    wandb.init(\n",
                "        project=\"fit-toy-model\",\n",
                "    )\n",
                "\n",
                "    try:\n",
                "        for unique_val in tqdm(unique_vals):\n",
                "            run = df.loc[df[unique_col] == unique_val]\n",
                "            rescaled_run = rescale_run(run, new_max=max_time)\n",
                "\n",
                "            pl_model = PatternLearningModel(\n",
                "                num_patterns=num_patterns, \n",
                "                max_time=max_time\n",
                "            )\n",
                "\n",
                "            def _plot_patterns(pl_model):\n",
                "                plot_patterns(pl_model, rescaled_run)\n",
                "                plt.show()\n",
                "\n",
                "            pl_model.fit(rescaled_run, lr=lr, num_epochs=num_epochs, callback=_plot_patterns)\n",
                "            pl_model.rescale(1.)\n",
                "\n",
                "            wandb.log({unique_col: unique_val, **pl_model.to_dict(), **hyperparams, **kwargs})\n",
                "\n",
                "            _plot_patterns(pl_model)\n",
                "            plt.show()\n",
                "\n",
                "    except KeyboardInterrupt:\n",
                "        wandb.finish()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fit the sweeps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "WD_SWEEP_ID = \"ib21hnk1\"\n",
                "LN_SWEEP_ID = \"8783j1j4\"\n",
                "DM_SWEEP_ID = \"l1b2mmci\"\n",
                "\n",
                "dm_sweep_2 = pd.read_csv(\"../unifying/mw_sweep.csv\")\n",
                "\n",
                "SWEEP_IDS = [\"mw-other\"] # [WD_SWEEP_ID, LN_SWEEP_ID, DM_SWEEP_ID]\n",
                "UNIQUE_COLS = [\"d_model\"] # [\"weight_decay\", \"frac_label_noise\", \"d_model\"]\n",
                "SWEEPS = [dm_sweep_2] # [get_history(sweep_id, unique_cols=unique_col) for sweep_id, unique_col in zip(SWEEP_IDS, UNIQUE_COLS)]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dm_sweep_2.fillna(method=\"ffill\", inplace=True)\n",
                "dm_sweep_2.loc[0, :].to_dict()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for sweep, sweep_id, unique_col in zip(SWEEPS, SWEEP_IDS, UNIQUE_COLS): \n",
                "    fit_sweep(sweep, unique_col, num_patterns=3, max_time=100.0, log=True, sweep=sweep_id, num_epochs=500)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "api = wandb.Api()\n",
                "runs = api.runs(f\"{ENTITY}/fit-toy-model\")\n",
                "[run for run in runs]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mw_fit_run_id = \"1gthqq5\"\n",
                "run = runs[0]\n",
                "df = run.history()\n",
                "\n",
                "col = \"d_model\"\n",
                "unique_vals = df.loc[:, col].unique()\n",
                "unique_vals"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "D_MODEL = 115\n",
                "\n",
                "model_entry = df.loc[df[col] == D_MODEL, :].iloc[0, :]\n",
                "model_entry"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def df_row_to_toy_model(row):\n",
                "    model = PatternLearningModel(max_time=100.)\n",
                "\n",
                "    for i, pattern in enumerate(model.patterns):\n",
                "        pattern.onset.data = torch.tensor(row[f\"pattern_{i}/onset\"])\n",
                "        pattern.speed.data = torch.tensor(row[f\"pattern_{i}/speed\"])\n",
                "        pattern.strength.data = torch.tensor(row[f\"pattern_{i}/strength\"])\n",
                "        pattern.generalization.data = torch.tensor(row[f\"pattern_{i}/generalization\"])\n",
                "\n",
                "    return model\n",
                "\n",
                "model = df_row_to_toy_model(model_entry)\n",
                "model.rescale(100)\n",
                "model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get corresponding original run\n",
                "og_df = get_history(DM_SWEEP_ID, unique_cols=\"d_model\")\n",
                "run = og_df.loc[og_df.d_model==D_MODEL,:] #.plot(x=\"_step\", y=\"test/acc\")\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
                "\n",
                "ax1.plot(run[\"_step\"], run[\"test/acc\"], label=\"Test\", color=RED, linewidth=2)\n",
                "ax1.plot(run[\"_step\"], run[\"train/acc\"], label=\"Train\", color=BLUE, linewidth=2)\n",
                "ax1.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "ax1.set_xlabel(\"Step\", fontsize=18)\n",
                "ax1.set_xscale(\"log\")\n",
                "ax1.legend()\n",
                "ax1.set_title(\"Truth\")\n",
                "\n",
                "min_step, max_step = og_df[\"_step\"].min(), 10000 # run[\"_step\"].max()\n",
                "\n",
                "ts = np.linspace(min_step, max_step, 1000)\n",
                "train_ys = [model(t).detach().numpy() for t in ts]\n",
                "test_ys = [model.test(t).detach().numpy() for t in ts]\n",
                "ax2.plot(ts, train_ys, label=\"Train\", color=BLUE, linewidth=2)\n",
                "ax2.plot(ts, test_ys, label=\"Test\", color=RED, linewidth=2)\n",
                "ax2.set_ylabel(\"Accuracy\", fontsize=18)\n",
                "ax2.set_xlabel(\"Step\", fontsize=18)\n",
                "ax2.set_title(\"Fit\")\n",
                "\n",
                "# Already in log scale\n",
                "# train_ys, test_ys"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ignore any d_model < 50\n",
                "df_cleaned = df.loc[df[\"d_model\"] >= 50, :]\n",
                "d_models = df_cleaned.loc[:, \"d_model\"].unique()\n",
                "\n",
                "# Scaling analysis\n",
                "fig = plt.figure(figsize=(15, 4))\n",
                "ax = fig.add_subplot(111)\n",
                "\n",
                "colors = [BLUE, RED, \"green\"]\n",
                "y_max = 0\n",
                "\n",
                "for i in range(3):\n",
                "    slice = df_cleaned.loc[:, f\"pattern_{i}/onset\"]\n",
                "    y_max = max(y_max, slice.max())\n",
                "    ax.plot(d_models, slice, label=f\"\", color=colors[i], linewidth=2)\n",
                "\n",
                "ax.set_xlabel(\"d_model\", fontsize=18)\n",
                "ax.set_ylabel(\"Onset\", fontsize=18)\n",
                "\n",
                "\n",
                "# Fit a power-law to the onsets \n",
                "from scipy.optimize import curve_fit\n",
                "\n",
                "def power_law(x, a, b):\n",
                "    return a * x**b\n",
                "\n",
                "def fit_power_law(x, y):\n",
                "    popt, pcov = curve_fit(power_law, x, y)\n",
                "    return popt\n",
                "\n",
                "\n",
                "CUTOFF = 175\n",
                "\n",
                "# Fit power law to onset\n",
                "for i in range(3):\n",
                "    # Train up to a specific point\n",
                "    df_to_fit = df_cleaned.loc[df_cleaned[\"d_model\"] <= CUTOFF, :]\n",
                "    d_models_to_fit = df_to_fit.loc[:, \"d_model\"].unique()\n",
                "\n",
                "    onset_popt = fit_power_law(d_models_to_fit, df_to_fit.loc[:, f\"pattern_{i}/onset\"])\n",
                "    exponent = round(onset_popt[1], 2)\n",
                "    ax.plot(d_models, power_law(d_models, *onset_popt), label=f\"$\\\\nu_{i} = {exponent}$\", color=colors[i], linestyle=\"--\", linewidth=2)\n",
                "\n",
                "ax.vlines(CUTOFF, 0, y_max * 1.05, color=\"grey\", linestyle=\"--\", linewidth=2)\n",
                "ax.set_xlabel(\"Embedding dim.\", fontsize=18)\n",
                "ax.set_ylim(0, y_max * 1.05)\n",
                "\n",
                "ax.legend()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's see if we can fit "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
