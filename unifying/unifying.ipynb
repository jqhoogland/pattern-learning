{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (1.24.2)\n",
      "Requirement already satisfied: torch in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: sympy in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (1.11.1)\n",
      "Requirement already satisfied: mod in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: blobfile in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: wheel in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: setuptools in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from sympy) (1.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from blobfile) (1.26.15)\n",
      "Requirement already satisfied: lxml~=4.9 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from blobfile) (4.9.2)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from blobfile) (3.17)\n",
      "Requirement already satisfied: filelock~=3.0 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from blobfile) (3.12.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy torch sympy mod blobfile\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "from contextlib import suppress\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import Callable, Literal, Optional, Union, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from grokking.dataset import ModularArithmetic, Operator\n",
    "from grokking.transformer import Transformer\n",
    "from grokking.utils import generate_run_name\n",
    "from grokking.learner import Config, GrokkingLearner\n",
    "\n",
    "DEFAULT_MODULUS = 113\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying Grokking & DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    lr=1e-3,\n",
    "    d_model=128,\n",
    "    weight_decay=1.,\n",
    "    test_acc_criterion=1.,\n",
    "    device=DEVICE,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "train_dataset, val_dataset = ModularArithmetic.generate_split(\n",
    "    operator=config.operator,\n",
    "    modulus=config.modulus,\n",
    "    frac_label_noise=config.frac_label_noise,\n",
    "    seed=config.seed,\n",
    "    shuffle=config.shuffle,\n",
    "    frac_train=config.frac_train,\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "date_time = datetime.now().strftime(\"%Y%m%d-%H%M%S-%f\")\n",
    "mode = \"disabled\" if config.no_logging else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 226816 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjqhoogland\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adc49d1f2a1427bbd9fe2cb63e85d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666992491664132, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/Projects/quanta-learning/unifying/wandb/run-20230421_145904-20230421-145900-721542</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jqhoogland/grokking/runs/20230421-145900-721542' target=\"_blank\">L1_H4_D128_V114_M512_d32_C3_lr0.001_wd1.0_AdamW_mom0.9_0.98_733f5562</a></strong> to <a href='https://wandb.ai/jqhoogland/grokking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jqhoogland/grokking' target=\"_blank\">https://wandb.ai/jqhoogland/grokking</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jqhoogland/grokking/runs/20230421-145900-721542' target=\"_blank\">https://wandb.ai/jqhoogland/grokking/runs/20230421-145900-721542</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710e598d223f4a48af9b7fda21f5cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy criterion reached\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "def train_test_run():\n",
    "    learner = GrokkingLearner.create(config, train_dataloader, val_dataloader)\n",
    "\n",
    "    if config.resume_run_id is None:\n",
    "        wandb.init(\n",
    "            project=config.wandb_project,\n",
    "            id=date_time,\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            name=learner.name,\n",
    "            config=asdict(config),\n",
    "            mode=mode,\n",
    "        )\n",
    "    else:\n",
    "        wandb.init(\n",
    "            project=config.wandb_project,\n",
    "            id=config.resume_run_id,\n",
    "            resume=\"must\",\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            name=learner.name,\n",
    "            config=asdict(config),\n",
    "            mode=mode,\n",
    "        )\n",
    "    wandb.watch(learner.model)\n",
    "    \n",
    "    try: \n",
    "        learner.train()\n",
    "    except KeyboardInterrupt:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "train_test_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweeps\n",
    "\n",
    "To initialize a sweep, run the following command:\n",
    "\n",
    "```shell\n",
    "wandb sweep --project grokking <config.yml>\n",
    "```\n",
    "\n",
    "where `<config.yml>` is the config file you want to use.\n",
    "\n",
    "To run the sweep, run the following command:\n",
    "\n",
    "```shell\n",
    "wandb agent <sweep_id> --function train\n",
    "```\n",
    "\n",
    "where `<sweep_id>` is the id of the sweep you want to run. You can find the sweep id by running `wandb sweep ls`.\n",
    "\n",
    "You can pass an optional `--count` flag to the `wandb agent` command to specify the number of runs you want to execute. If you don't pass this flag, the agent will run until all the runs in the sweep are complete (for a grid sweep).\n",
    "\n",
    "On a multi-GPU machine, you can run multiple agents in parallel through the following:\n",
    "\n",
    "```shell\n",
    "CUDA_VISIBLE_DEVICES=0 wandb agent <sweep_id> &\n",
    "CUDA_VISIBLE_DEVICES=1 wandb agent <sweep_id> &\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def generate_coarse_to_fine_grid_sweep(min_, max_, total_steps, step_sizes=[10, 5, 3, 1], type_=\"log\"):\n",
    "    if type_ == \"log\":\n",
    "        # Generate the logscale range\n",
    "        grid = np.logspace(np.log10(min_), np.log10(max_), total_steps)\n",
    "    elif type_ == \"linear\":\n",
    "        grid = np.linspace(min_, max_, total_steps)\n",
    "    else:\n",
    "        grid = np.arange(min_, max_, int((max_ - min_) / total_steps))\n",
    "\n",
    "    # Initialize an empty list to store the rearranged elements\n",
    "    rearranged_grid = []\n",
    "\n",
    "    # Iterate over the step sizes and merge the sublists\n",
    "    for step in step_sizes:\n",
    "        for i in range(0, len(grid), step):\n",
    "            if grid[i] not in rearranged_grid:\n",
    "                rearranged_grid.append(grid[i])\n",
    "\n",
    "    return rearranged_grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 80, 140, 200, 50, 110, 170, 40, 60, 100, 120, 160, 180, 30, 70, 90, 130, 150, 190, 25, 35, 45, 55, 65, 75, 85, 95, 105, 115, 125, 135, 145, 155, 165, 175, 185, 195, 23, 26, 29, 32, 38, 41, 44, 47, 53, 56, 59, 62, 68, 71, 74, 77, 83, 86, 89, 92, 98, 101, 104, 107, 113, 116, 119, 122, 128, 131, 134, 137, 143, 146, 149, 152, 158, 161, 164, 167, 173, 176, 179, 182, 188, 191, 194, 197, 21, 22, 24, 27, 28, 31, 33, 34, 36, 37, 39, 42, 43, 46, 48, 49, 51, 52, 54, 57, 58, 61, 63, 64, 66, 67, 69, 72, 73, 76, 78, 79, 81, 82, 84, 87, 88, 91, 93, 94, 96, 97, 99, 102, 103, 106, 108, 109, 111, 112, 114, 117, 118, 121, 123, 124, 126, 127, 129, 132, 133, 136, 138, 139, 141, 142, 144, 147, 148, 151, 153, 154, 156, 157, 159, 162, 163, 166, 168, 169, 171, 172, 174, 177, 178, 181, 183, 184, 186, 187, 189, 192, 193, 196, 198, 199]\n"
     ]
    }
   ],
   "source": [
    "model_grid = generate_coarse_to_fine_grid_sweep(20, 201, 100, step_sizes=[60, 30, 20, 10, 5, 3, 1], type_=\"range\")\n",
    "print(model_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.049999999999999996, 0.14426999059072135, 0.41627660370093655, 1.201124433981431, 3.4657242157757318, 10.0, 0.08493232323171235, 0.24506370946974493, 0.7071067811865475, 2.0402857733683692, 5.887040186524747, 0.06871187569715699, 0.09442643723643111, 0.12976435235830103, 0.17832704098331334, 0.3367757428593863, 0.46280985962343724, 0.6360106709172864, 0.87402972324268, 1.6506302560910038, 2.2683580195698294, 3.1172626855466286, 4.283859323293314, 8.090191470413135, 0.05558922306812267, 0.06180323442635004, 0.07639279571116754, 0.10498184566128109, 0.1167171847313636, 0.16039713377967135, 0.19826123320599312, 0.22042375836898087, 0.27245802423230514, 0.3029145977149917, 0.37442203787486295, 0.5145448104946759, 0.572062924982669, 0.7861503318472239, 0.9717326650701373, 1.0803572776233041, 1.3353914818633272, 1.484667499371428, 1.8351450701767056, 2.5219251989646447, 2.803837248927306, 3.8531383304670337, 4.7627282303001826, 5.29512724014004, 6.545119802794536, 7.2767624945026474, 8.994549166252378]\n"
     ]
    }
   ],
   "source": [
    "wds = generate_coarse_to_fine_grid_sweep(0.05, 10, 51)\n",
    "print(json.dumps(wds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we induce grokking in CIFAR-10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# import cifar10    \n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from grokking.learner import BaseLearner\n",
    "\n",
    "cifar_train = CIFAR10(root=\"../data\", train=True, download=True)\n",
    "cifar_test = CIFAR10(root=\"../data\", train=False, download=True)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_channels, kernel_size=3, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=kernel_size, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=kernel_size, padding=1)\n",
    "        \n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        if self.conv3:\n",
    "            x = self.conv3(x)\n",
    "        \n",
    "        out += x\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_blocks: int,\n",
    "        num_classes: int,\n",
    "        in_channels: int = 3,\n",
    "        in_size: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_size = in_size\n",
    "        self.in_channels = in_channels\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=5, stride=2, padding=0, bias=False)\n",
    "        size = (in_size - 5) // 2 + 1\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        size = (size - 3) // 2 + 1\n",
    "\n",
    "        resblocks = [\n",
    "            ResBlock(32 * (2 ** i), 32 * (2 ** (i + 1)), strides=2, kernel_size=3),\n",
    "            for i in range(num_blocks)\n",
    "        ]\n",
    "\n",
    "        for i, _ in enumerate(resblocks):\n",
    "            size = (size - 3) // 2 + 1\n",
    "\n",
    "        self.resblocks = nn.Sequential(*resblocks)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        num_channels = 32 * (2 ** num_blocks)\n",
    "        self.fc1 = nn.Linear(num_channels * size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "\n",
    "        out = self.maxpool(out)\n",
    "        out = self.resblocks(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CIFARConfig(Config):\n",
    "    num_blocks: int = 2\n",
    "    num_classes: int = 10\n",
    "    in_channels: int = 3\n",
    "    in_size: int = 32\n",
    "\n",
    "    # Dataset\n",
    "    frac_train: float = 0.2\n",
    "    frac_label_noise: float = 0.0\n",
    "\n",
    "\n",
    "class CIFARLearner(BaseLearner):\n",
    "    Config = CIFARConfig\n",
    "    Dataset = Union[CIFAR10, Subset[CIFAR10]]\n",
    "\n",
    "    @classmethod\n",
    "    def create(\n",
    "        cls,\n",
    "        config: Config,\n",
    "        trainset: Dataset,\n",
    "        testset: Dataset,\n",
    "    ) -> \"BaseLearner\":\n",
    "        model = cls.get_model(config)\n",
    "        optimizer = cls.get_optimizer(config, model)\n",
    "        trainloader = cls.get_loader(config, trainset)\n",
    "        testloader = cls.get_loader(config, testset, train=False)\n",
    "        return cls(model, optimizer, config, trainloader, testloader)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loader(config: Config, dataset: Dataset, train=True) -> DataLoader[Dataset]:\n",
    "        if train and config.frac_train < 1.0:\n",
    "            dataset = Subset(\n",
    "                dataset, \n",
    "                list(range(int(len(dataset) * config.frac_train)))\n",
    "            )\n",
    "\n",
    "        def add_label_noise(dataset: CIFARLearner.Dataset, frac_label_noise: float) -> CIFARLearner.Dataset:\n",
    "            num_samples = len(dataset)\n",
    "            num_errors = int(num_samples * frac_label_noise)\n",
    "            \n",
    "            origin_indices = torch.randperm(num_samples)[:num_errors]\n",
    "            target_indices = origin_indices.roll(1)\n",
    "\n",
    "            for origin, target in zip(origin_indices, target_indices):\n",
    "                dataset.targets[origin] = dataset.targets[target]  # TODO: Make this not in-place\n",
    "\n",
    "            return dataset\n",
    "\n",
    "        if config.frac_label_noise > 0.0:\n",
    "            dataset = add_label_noise(dataset, config.frac_label_noise)\n",
    "\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=train,\n",
    "        )\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we interpolate just by varying initialization scale and label noise?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we induce epoch-/regularization-wise DD in shallow models?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we induce epoch-wise DD in transformers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
