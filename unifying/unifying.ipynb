{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (1.24.2)\n",
      "Requirement already satisfied: torch in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: sympy in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (1.11.1)\n",
      "Requirement already satisfied: mod in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: blobfile in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: setuptools in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from sympy) (1.3.0)\n",
      "Requirement already satisfied: lxml~=4.9 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from blobfile) (4.9.2)\n",
      "Requirement already satisfied: filelock~=3.0 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from blobfile) (3.12.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from blobfile) (1.26.15)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /home/paperspace/Projects/quanta-learning/.venv/lib/python3.9/site-packages (from blobfile) (3.17)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy torch sympy mod blobfile\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "from contextlib import suppress\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import Callable, Literal, Optional, Union, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from grokking.dataset import ModularArithmetic, Operator\n",
    "from grokking.transformer import Transformer\n",
    "from grokking.utils import generate_run_name\n",
    "from grokking.validation import criterion, validate\n",
    "from grokking.learner import Config, GrokkingLearner\n",
    "\n",
    "DEFAULT_MODULUS = 113\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying Grokking & DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    lr=1e-3,\n",
    "    d_model=128,\n",
    "    weight_decay=0.1,\n",
    "    test_acc_criterion=1.,\n",
    "    device=DEVICE,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "train_dataset, val_dataset = ModularArithmetic.generate_split(\n",
    "    operator=config.operator,\n",
    "    modulus=config.modulus,\n",
    "    frac_label_noise=config.frac_label_noise,\n",
    "    seed=config.seed,\n",
    "    shuffle=config.shuffle,\n",
    "    frac_train=config.frac_train,\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "date_time = datetime.now().strftime(\"%Y%m%d-%H%M%S-%f\")\n",
    "mode = \"disabled\" if config.no_logging else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 226816 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjqhoogland\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/paperspace/Projects/quanta-learning/unifying/wandb/run-20230421_122851-20230421-122847-441518</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jqhoogland/grokking/runs/20230421-122847-441518' target=\"_blank\">L1_H4_D128_V114_M512_d32_C3_lr0.001_wd0.1_AdamW_mom0.9_0.98_195fd97c</a></strong> to <a href='https://wandb.ai/jqhoogland/grokking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jqhoogland/grokking' target=\"_blank\">https://wandb.ai/jqhoogland/grokking</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jqhoogland/grokking/runs/20230421-122847-441518' target=\"_blank\">https://wandb.ai/jqhoogland/grokking/runs/20230421-122847-441518</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e023b21181847799789f4c9c4b0b336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "def train_test_run():\n",
    "    learner = GrokkingLearner.create(config, train_dataloader, val_dataloader)\n",
    "\n",
    "    if config.resume_run_id is None:\n",
    "        wandb.init(\n",
    "            project=config.wandb_project,\n",
    "            id=date_time,\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            name=learner.name,\n",
    "            config=asdict(config),\n",
    "            mode=mode,\n",
    "        )\n",
    "    else:\n",
    "        wandb.init(\n",
    "            project=config.wandb_project,\n",
    "            id=config.resume_run_id,\n",
    "            resume=\"must\",\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            name=learner.name,\n",
    "            config=asdict(config),\n",
    "            mode=mode,\n",
    "        )\n",
    "    wandb.watch(learner.model)\n",
    "    \n",
    "    try: \n",
    "        learner.train()\n",
    "    except KeyboardInterrupt:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "train_test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: fr8286r5\n",
      "Sweep URL: https://wandb.ai/jqhoogland/dominoes/sweeps/fr8286r5\n",
      "WANDB sweep name: sweep-7382929675824336860\n",
      "WANDB sweep ID: fr8286r5\n"
     ]
    }
   ],
   "source": [
    "# Sweep\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'name': 'sweep',\n",
    "    'parameters': {\n",
    "        k : {\"value\": v} for k, v in asdict(config).items()\n",
    "    } | {\n",
    "        'd_model': {\n",
    "            'values': [32, 64, 128, 256, 512]\n",
    "        },\n",
    "        'seed': {\n",
    "            'values': [0, 1, 2, 3, 4]\n",
    "        },\n",
    "        # TODO: Look at initial gain, frac_train, frac_label_noise\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_sweep_info(sweep_config):\n",
    "    # Hash the config (make sure to alphabetize the keys)\n",
    "    config_hash = hash(str(sorted(sweep_config.items())))        \n",
    "    sweep_config['name'] = f'{sweep_config[\"name\"]}-{config_hash}'\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"dominoes\")\n",
    "\n",
    "    return sweep_config['name'], sweep_id\n",
    " \n",
    "\n",
    "sweep_name, sweep_id = get_sweep_info(sweep_config)\n",
    "\n",
    "print(\"WANDB sweep name:\", sweep_name)\n",
    "print(\"WANDB sweep ID:\", sweep_id)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep function\n",
    "\n",
    "def train():\n",
    "    wandb.init(project=\"dominoes\")\n",
    "    config_dict = wandb.config \n",
    "    config = Config(**config_dict)\n",
    "\n",
    "    learner = GrokkingLearner.create(\n",
    "        config=config,\n",
    "        trainloader=train_dataloader,\n",
    "        testloader=val_dataloader,\n",
    "    )\n",
    "\n",
    "    # wandb.watch(learner.model)\n",
    "    metrics = learner.train()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=train, count=10 * 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we induce grokking in CIFAR-10?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we interpolate just by varying initialization scale and label noise?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we induce epoch-/regularization-wise DD in shallow models?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we induce epoch-wise DD in transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
