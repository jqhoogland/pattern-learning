{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (1.24.2)\n",
      "Requirement already satisfied: torch in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: sympy in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (1.11.1)\n",
      "Requirement already satisfied: mod in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (0.3.0)\n",
      "Requirement already satisfied: blobfile in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (from sympy) (1.3.0)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (from blobfile) (3.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (from blobfile) (1.26.15)\n",
      "Requirement already satisfied: lxml~=4.9 in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (from blobfile) (4.9.2)\n",
      "Requirement already satisfied: filelock~=3.0 in /Users/Jesse/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages (from blobfile) (3.12.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy torch sympy mod blobfile\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "from contextlib import suppress\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from typing import Callable, Literal, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import wandb\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from grokking.dataset import DEFAULT_MODULUS, ModularArithmetic, Operator\n",
    "from grokking.transformer import Transformer\n",
    "from grokking.utils import generate_run_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying Grokking & DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 288256 trainable parameters.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:20230421-114626-299586) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436f39b660ad461c9b942f25148826e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">L1_H4_D128_V98_M512_d64_C3_lr0.001_wd1e-05_Adam_mom(0.9, 0.98)_hash9f389163</strong> at: <a href='https://wandb.ai/jqhoogland/grokking/runs/20230421-114626-299586' target=\"_blank\">https://wandb.ai/jqhoogland/grokking/runs/20230421-114626-299586</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230421_114626-20230421-114626-299586/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:20230421-114626-299586). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c344c052474be495a19ea5d421f49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670592183315118, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Jesse/Projects/unifying-grok-dd/unifying/wandb/run-20230421_115021-20230421-115021-106716</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jqhoogland/grokking/runs/20230421-115021-106716' target=\"_blank\">L1_H4_D128_V98_M512_d64_C3_lr0.001_wd1e-05_Adam_mom(0.9, 0.98)_hash9f389163</a></strong> to <a href='https://wandb.ai/jqhoogland/grokking' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jqhoogland/grokking' target=\"_blank\">https://wandb.ai/jqhoogland/grokking</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jqhoogland/grokking/runs/20230421-115021-106716' target=\"_blank\">https://wandb.ai/jqhoogland/grokking/runs/20230421-115021-106716</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56c61cbf4794b719a5e5ac4551c58e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 229\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mfor\u001b[39;00m i, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    227\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m config\u001b[39m.\u001b[39mlog_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    228\u001b[0m         wandb\u001b[39m.\u001b[39mlog(\n\u001b[0;32m--> 229\u001b[0m             validate(model, train_dataloader, val_dataloader, criterion),\n\u001b[1;32m    230\u001b[0m             step\u001b[39m=\u001b[39mstep\n\u001b[1;32m    231\u001b[0m         )\n\u001b[1;32m    233\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    234\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(DEVICE), y\u001b[39m.\u001b[39mto(DEVICE)\n",
      "Cell \u001b[0;32mIn[4], line 208\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, trainloader, testloader, criterion)\u001b[0m\n\u001b[1;32m    203\u001b[0m         norm_squared \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mnorm()\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m norm_squared\u001b[39m.\u001b[39msqrt()\n\u001b[0;32m--> 208\u001b[0m train_loss, train_acc \u001b[39m=\u001b[39m _validate(trainloader)  \n\u001b[1;32m    209\u001b[0m test_loss, test_acc \u001b[39m=\u001b[39m _validate(testloader)\n\u001b[1;32m    210\u001b[0m weight_norm \u001b[39m=\u001b[39m _weight_norm(model)\n",
      "Cell \u001b[0;32mIn[4], line 189\u001b[0m, in \u001b[0;36mvalidate.<locals>._validate\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m    188\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(DEVICE), y\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 189\u001b[0m     y_hat \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m    190\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m criterion(y_hat, y, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    192\u001b[0m     y_pred \u001b[39m=\u001b[39m y_hat\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/unifying-grok-dd/grokking/transformer.py:133\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed(x)\n\u001b[1;32m    132\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 133\u001b[0m     x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m    134\u001b[0m \u001b[39m# x = self.ln(x)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munembed(x)\n",
      "File \u001b[0;32m~/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/unifying-grok-dd/grokking/transformer.py:110\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    109\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(x)\n\u001b[0;32m--> 110\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp((x))\n\u001b[1;32m    111\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/unifying-grok-dd/grokking/transformer.py:93\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 93\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m'\u001b[39;49m\u001b[39mmd,bpd->bpm\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_in, x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_in\n\u001b[1;32m     94\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(x)                \n\u001b[1;32m     95\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mdm,bpm->bpd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_out, x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_out\n",
      "File \u001b[0;32m~/Projects/unifying-grok-dd/.venv/lib/python3.10/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model\n",
    "    num_layers: int = 1\n",
    "    num_heads: int = 4\n",
    "    d_model: int = 128\n",
    "    d_vocab: int = DEFAULT_MODULUS + 1\n",
    "    d_mlp: int = 4 * 128 # 4 * d_model\n",
    "    d_head: int = 64 # d_model // num_heads\n",
    "    num_ctx: int = 3\n",
    "    act_fn: Callable = F.relu\n",
    "    load_path: Optional[str] = None\n",
    "    # use_ln: bool = True\n",
    "\n",
    "    # Dataset\n",
    "    operator: Operator = \"+\"\n",
    "    modulus: int = DEFAULT_MODULUS  \n",
    "    frac_label_noise: float = 0.0\n",
    "    seed: int = 0         \n",
    "    shuffle: bool = True\n",
    "    frac_train: float = 0.3\n",
    "\n",
    "    # Dataloaders\n",
    "    batch_size: int = 64\n",
    "\n",
    "    # Optimizer\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    use_sgd: bool = False\n",
    "    momentum: float | tuple[float, float] = (0.9, 0.98)\n",
    "\n",
    "    # Training\n",
    "    num_training_steps: int = int(3e5)\n",
    "    num_jobs: int = 1\n",
    "\n",
    "    # Logging\n",
    "    wandb_project: str = \"grokking\"\n",
    "    no_logging: bool = False\n",
    "    resume_run_id: Optional[str] = None\n",
    "    log_normalized_loss: bool = True\n",
    "    log_interval: int = 10\n",
    "\n",
    "    weights_dir: str = \"weights\"\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Model\n",
    "\n",
    "model = (\n",
    "    Transformer(\n",
    "        num_layers=config.num_layers,\n",
    "        num_heads=config.num_heads,\n",
    "        d_model=config.d_model,\n",
    "        d_vocab=config.d_vocab,\n",
    "        d_mlp=config.d_mlp,\n",
    "        d_head=config.d_head,\n",
    "        num_ctx=config.num_ctx,\n",
    "        act_fn=config.act_fn,\n",
    "        # use_ln=config.use_ln,\n",
    "    )\n",
    "    .float()\n",
    "    .to(DEVICE)\n",
    ")\n",
    "\n",
    "if config.load_path is not None:\n",
    "    model.load_state_dict(torch.load(config.load_path))\n",
    "\n",
    "with suppress(FileExistsError):\n",
    "    os.makedirs(config.weights_dir)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "print(f\"Model has {num_params} trainable parameters.\")\n",
    "\n",
    "# Dataset\n",
    "\n",
    "train_dataset, val_dataset = ModularArithmetic.generate_split(\n",
    "    operator=config.operator,\n",
    "    modulus=config.modulus,\n",
    "    frac_label_noise=config.frac_label_noise,\n",
    "    seed=config.seed,\n",
    "    shuffle=config.shuffle,\n",
    ")\n",
    "\n",
    "# Dataloaders\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size)\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "if config.use_sgd and not isinstance(config.momentum, tuple):\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        weight_decay=config.weight_decay,\n",
    "        momentum=config.momentum,\n",
    "    )\n",
    "elif isinstance(config.momentum, tuple):\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        weight_decay=config.weight_decay,\n",
    "        betas=config.momentum,\n",
    "    )\n",
    "else: \n",
    "    raise ValueError(\"Invalid optimizer configuration.\")\n",
    "\n",
    "# Logging\n",
    "name = generate_run_name(\n",
    "    asdict(config),\n",
    "    aliases={\n",
    "        \"num_layers\": \"L\",\n",
    "        \"num_heads\": \"H\",\n",
    "        \"d_model\": \"D\",\n",
    "        \"d_vocab\": \"V\",\n",
    "        \"d_mlp\": \"M\",\n",
    "        \"d_head\": \"d\",\n",
    "        \"num_ctx\": \"C\",\n",
    "        \"lr\": \"lr\",\n",
    "        \"weight_decay\": \"wd\",\n",
    "        \"momentum\": \"mom\",\n",
    "    },\n",
    "    bool_aliases={\n",
    "        \"use_sgd\": {True: \"SGD\", False: \"Adam\"},\n",
    "    },\n",
    "    append_hash=True,\n",
    ")\n",
    "date_time = datetime.now().strftime(\"%Y%m%d-%H%M%S-%f\")\n",
    "mode = \"disabled\" if config.no_logging else None\n",
    "\n",
    "if config.resume_run_id is None:\n",
    "    wandb.init(\n",
    "        project=config.wandb_project,\n",
    "        id=date_time,\n",
    "        settings=wandb.Settings(start_method=\"thread\"),\n",
    "        name=name,\n",
    "        config=asdict(config),\n",
    "        mode=mode,\n",
    "    )\n",
    "else:\n",
    "    wandb.init(\n",
    "        project=config.wandb_project,\n",
    "        id=config.resume_run_id,\n",
    "        resume=\"must\",\n",
    "        settings=wandb.Settings(start_method=\"thread\"),\n",
    "        name=name,\n",
    "        config=asdict(config),\n",
    "        mode=mode,\n",
    "    )\n",
    "wandb.watch(model)\n",
    "\n",
    "# Training\n",
    "Reduction = Literal[\"mean\", \"sum\"]\n",
    "\n",
    "def criterion(logits, labels, reduction: Reduction=\"sum\"):\n",
    "    # only look at predictions of last numbers\n",
    "    logits = logits[:,-1]\n",
    "    # compute individual and summed losses for final number\n",
    "    logprobs = F.log_softmax(logits.to(torch.float64), dim=-1)\n",
    "    prediction_logprobs = torch.gather(logprobs, index=labels.unsqueeze(1), dim=-1)\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = -torch.mean(prediction_logprobs)\n",
    "    elif reduction == \"sum\":\n",
    "        loss = -torch.sum(prediction_logprobs)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid reduction argument.\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "steps_per_epoch = len(train_dataloader)\n",
    "step = 0\n",
    "\n",
    "def validate(model: nn.Module, trainloader: DataLoader, testloader: DataLoader, criterion: nn.Module):\n",
    "    model.eval()\n",
    "\n",
    "    def _validate(loader):\n",
    "        loss = torch.zeros(1)\n",
    "        acc = torch.zeros(1)\n",
    "\n",
    "        num_samples = len(loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                y_hat = model(x)\n",
    "                loss += criterion(y_hat, y, reduction=\"sum\")\n",
    "                \n",
    "                y_pred = y_hat.argmax(dim=-1)[:, -1].detach()\n",
    "                acc += (y == y_pred).float().sum()\n",
    "\n",
    "        loss /= num_samples\n",
    "        acc = acc / num_samples\n",
    "\n",
    "        return loss, acc\n",
    "\n",
    "    def _weight_norm(model):\n",
    "        norm_squared = torch.zeros(1)\n",
    "        for p in model.parameters():\n",
    "            norm_squared += p.norm().pow(2)\n",
    "\n",
    "        return norm_squared.sqrt()\n",
    "\n",
    "\n",
    "    train_loss, train_acc = _validate(trainloader)  \n",
    "    test_loss, test_acc = _validate(testloader)\n",
    "    weight_norm = _weight_norm(model)\n",
    "    \n",
    "    # Efficiency is logprob of the correct label divided by the norm of the weights\n",
    "\n",
    "    return {\n",
    "        \"train/loss\": train_loss.item(),\n",
    "        \"train/acc\": train_acc.item(),\n",
    "        \"train/efficiency\": (train_loss / weight_norm).item(),\n",
    "        \"test/loss\": test_loss.item(),                \n",
    "        \"test/acc\": test_acc.item(),\n",
    "        \"test/efficiency\": (test_loss / weight_norm).item(),\n",
    "        \"weight_norm\": weight_norm.item(),\n",
    "    }\n",
    "\n",
    "# train\n",
    "for epoch in tqdm(range(1, int(config.num_training_steps / steps_per_epoch) + 1)):\n",
    "    for i, (x, y) in enumerate(train_dataloader):\n",
    "        if step % config.log_interval == 0:\n",
    "            wandb.log(\n",
    "                validate(model, train_dataloader, val_dataloader, criterion),\n",
    "                step=step\n",
    "            )\n",
    "\n",
    "        model.train()\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y, reduction=\"mean\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization-wise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we induce grokking in CIFAR-10?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we interpolate just by varying initialization scale and label noise?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we induce epoch-/regularization-wise DD in shallow models?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we induce epoch-wise DD in transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
